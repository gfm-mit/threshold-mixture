{"notes":[{"content":{"title":{"value":"Simple linear attention language models balance the recall-throughput tradeoff"},"authors":{"value":["Simran Arora","Sabri Eyuboglu","Michael Zhang","Aman Timalsina","Silas Alberti","James Zou","Atri Rudra","Christopher Re"]},"authorids":{"value":["~Simran_Arora1","~Sabri_Eyuboglu1","~Michael_Zhang4","~Aman_Timalsina1","~Silas_Alberti2","~James_Zou1","~Atri_Rudra1","~Christopher_Re1"]},"abstract":{"value":"Recent work has shown that attention-based language models excel at \"recall\", the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's recurrent state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the Pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to $1.3$b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 10.36 accuracy points. We further develop IO-aware algorithms that enable BASED to provide 24× higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Overall, BASED expands the Pareto frontier of the throughput-recall tradeoff space beyond prior architectures."},"pdf":{"value":"/pdf/8007385a20b06d26728f6b395b6dfbda50253ca6.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\narora2024simple,\ntitle={Simple linear attention language models balance the recall-throughput tradeoff},\nauthor={Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and James Zou and Atri Rudra and Christopher Re},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=e93ffDcpH3}\n}"},"paperhash":{"value":"arora|simple_linear_attention_language_models_balance_the_recallthroughput_tradeoff"}},"id":"e93ffDcpH3","forum":"e93ffDcpH3","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9942/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9942/Authors"],"number":9942,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9942/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874560146,"cdate":1706874560146,"tmdate":1719287296749,"mdate":1719287296749,"pdate":1714610506383,"odate":1717693105157,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Allocation Requires Prediction Only if Inequality Is Low"},"authors":{"value":["Ali Shirali","Rediet Abebe","Moritz Hardt"]},"authorids":{"value":["~Ali_Shirali1","~Rediet_Abebe2","~Moritz_Hardt1"]},"abstract":{"value":"Algorithmic predictions are emerging as a promising solution concept for efficiently allocating societal resources. Fueling their use is an underlying assumption that such systems are necessary to identify individuals for interventions. We propose a principled framework for assessing this assumption: Using a simple mathematical model, we evaluate the efficacy of prediction-based allocations in settings where individuals belong to larger units such as hospitals, neighborhoods, or schools. We find that prediction-based allocations outperform baseline methods using aggregate unit-level statistics only when between-unit inequality is low and the intervention budget is high. Our results hold for a wide range of settings for the price of prediction, treatment effect heterogeneity, and unit-level statistics' learnability. Combined, we highlight the potential limits to improving the efficacy of interventions through prediction."},"pdf":{"value":"/pdf/9891ebefd1add09572e2f3c38ef0c80576b1ed4e.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nshirali2024allocation,\ntitle={Allocation Requires Prediction Only if Inequality Is Low},\nauthor={Ali Shirali and Rediet Abebe and Moritz Hardt},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=WUicA0hOF9}\n}"},"paperhash":{"value":"shirali|allocation_requires_prediction_only_if_inequality_is_low"}},"id":"WUicA0hOF9","forum":"WUicA0hOF9","license":"CC BY-NC-ND 4.0","signatures":["ICML.cc/2024/Conference/Submission9854/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9854/Authors"],"number":9854,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9854/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874223843,"cdate":1706874223843,"tmdate":1719287296112,"mdate":1719287296112,"pdate":1714610504248,"odate":1717693104501,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing"},"authors":{"value":["Hongbin Pei","Yu Li","Huiqi Deng","Jingxin Hai","Pinghui Wang","Jie Ma","Jing Tao","Yuheng Xiong","Xiaohong Guan"]},"authorids":{"value":["~Hongbin_Pei1","~Yu_Li30","~Huiqi_Deng1","haijingxin@stu.xjtu.edu.cn","~Pinghui_Wang1","~Jie_Ma1","~Jing_Tao2","~Yuheng_Xiong1","~Xiaohong_Guan2"]},"abstract":{"value":"The advancement toward deeper graph neural networks is currently obscured by two inherent issues in message passing, *oversmoothing* and *oversquashing*. We identify the root cause of these issues as information loss due to *heterophily mixing* in aggregation, where messages of diverse category semantics are mixed. We propose a novel multi-track graph convolutional network to address oversmoothing and oversquashing effectively. Our basic idea is intuitive: if messages are separated and independently propagated according to their category semantics, heterophilic mixing can be prevented. Consequently, we present a novel multi-track message passing scheme capable of preventing heterophilic mixing, enhancing long-distance information flow, and improving separation condition. Empirical validations show that our model achieved state-of-the-art performance on several graph datasets and effectively tackled oversmoothing and oversquashing, setting a new benchmark of $86.4$% accuracy on Cora."},"pdf":{"value":"/pdf/2dc1ec156d3862757eb963f76af9f18681a515ba.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\npei2024multitrack,\ntitle={Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing},\nauthor={Hongbin Pei and Yu Li and Huiqi Deng and Jingxin Hai and Pinghui Wang and Jie Ma and Jing Tao and Yuheng Xiong and Xiaohong Guan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=1sRuv4cnuZ}\n}"},"paperhash":{"value":"pei|multitrack_message_passing_tackling_oversmoothing_and_oversquashing_in_graph_learning_via_preventing_heterophily_mixing"}},"id":"1sRuv4cnuZ","forum":"1sRuv4cnuZ","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9699/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9699/Authors"],"number":9699,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9699/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873548305,"cdate":1706873548305,"tmdate":1719287295117,"mdate":1719287295117,"pdate":1714610500656,"odate":1717693103473,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models"},"authors":{"value":["Haotian Sun","Yuchen Zhuang","Wei Wei","Chao Zhang","Bo Dai"]},"authorids":{"value":["~Haotian_Sun1","~Yuchen_Zhuang1","~Wei_Wei15","~Chao_Zhang15","~Bo_Dai1"]},"abstract":{"value":"Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively."},"pdf":{"value":"/pdf/9eebbe03f032b9b23ee8b8af464cb9c4ebb9675c.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsun2024bboxadapter,\ntitle={{BB}ox-Adapter: Lightweight Adapting for Black-Box Large Language Models},\nauthor={Haotian Sun and Yuchen Zhuang and Wei Wei and Chao Zhang and Bo Dai},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jdRIaUu3xY}\n}"},"paperhash":{"value":"sun|bboxadapter_lightweight_adapting_for_blackbox_large_language_models"}},"id":"jdRIaUu3xY","forum":"jdRIaUu3xY","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9696/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9696/Authors"],"number":9696,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9696/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873537641,"cdate":1706873537641,"tmdate":1719287295062,"mdate":1719287295062,"pdate":1714610500564,"odate":1717693103398,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Position: Understanding LLMs Requires More Than Statistical Generalization"},"authors":{"value":["Patrik Reizinger","Szilvia Ujváry","Anna Mészáros","Anna Kerekes","Wieland Brendel","Ferenc Huszár"]},"authorids":{"value":["~Patrik_Reizinger1","~Szilvia_Ujváry1","~Anna_Mészáros1","~Anna_Kerekes1","~Wieland_Brendel1","~Ferenc_Huszár1"]},"abstract":{"value":"The last decade has seen blossoming research in deep learning theory attempting to answer, ``Why does deep learning generalize?\" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart---thus, equivalent test loss---can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases."},"pdf":{"value":"/pdf/406a57025ad6a094610cb5f066424ad0a2dcfd93.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nreizinger2024position,\ntitle={Position: Understanding {LLM}s Requires More Than Statistical Generalization},\nauthor={Patrik Reizinger and Szilvia Ujv{\\'a}ry and Anna M{\\'e}sz{\\'a}ros and Anna Kerekes and Wieland Brendel and Ferenc Husz{\\'a}r},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=pVyOchWUBa}\n}"},"paperhash":{"value":"reizinger|position_understanding_llms_requires_more_than_statistical_generalization"}},"id":"pVyOchWUBa","forum":"pVyOchWUBa","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9692/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9692/Authors"],"number":9692,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9692/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873514802,"cdate":1706873514802,"tmdate":1719287294997,"mdate":1719287294997,"pdate":1714610500453,"odate":1717693103332,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images"},"authors":{"value":["Baoying Chen","Jishen Zeng","Jianquan Yang","Rui Yang"]},"authorids":{"value":["~Baoying_Chen1","~Jishen_Zeng1","~Jianquan_Yang1","~Rui_Yang18"]},"abstract":{"value":"Diffusion models have made significant strides in visual content generation but also raised increasing demands on generated image detection. Existing detection methods have achieved considerable progress, but they usually suffer a significant decline in accuracy when detecting images generated by an unseen diffusion model. In this paper, we seek to address the generalizability of generated image detectors from the perspective of hard sample classification. The basic idea is that if a classifier can distinguish generated images that closely resemble real ones, then it can also effectively detect less similar samples, potentially even those produced by a different diffusion model. Based on this idea, we propose Diffusion Reconstruction Contrastive Learning (DRCT), a universal framework to enhance the generalizability of the existing detectors. DRCT generates hard samples by high-quality diffusion reconstruction and adopts contrastive training to guide the learning of diffusion artifacts. In addition, we have built a million-scale dataset, DRCT-2M, including 16 types diffusion models for the evaluation of generalizability of detection methods. Extensive experimental results show that detectors enhanced with DRCT achieve over a 10% accuracy improvement in cross-set tests. The code, models, and dataset will soon be available at https://github.com/beibuwandeluori/DRCT."},"pdf":{"value":"/pdf/6a65ad38d0c82d1a5b968eef583f28efb1c0a6bd.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nchen2024drct,\ntitle={{DRCT}: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images},\nauthor={Baoying Chen and Jishen Zeng and Jianquan Yang and Rui Yang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=oRLwyayrh1}\n}"},"paperhash":{"value":"chen|drct_diffusion_reconstruction_contrastive_training_towards_universal_detection_of_diffusion_generated_images"}},"id":"oRLwyayrh1","forum":"oRLwyayrh1","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9651/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9651/Authors"],"number":9651,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9651/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873256795,"cdate":1706873256795,"tmdate":1719287294798,"mdate":1719287294798,"pdate":1714610499405,"odate":1717693103097,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Explaining Probabilistic Models with Distributional Values"},"authors":{"value":["Luca Franceschi","Michele Donini","Cedric Archambeau","Matthias Seeger"]},"authorids":{"value":["~Luca_Franceschi1","~Michele_Donini1","~Cedric_Archambeau1","~Matthias_Seeger2"]},"abstract":{"value":"A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the *distributional values*, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models."},"pdf":{"value":"/pdf/2ac36966979d561bd323eb50ee7f6bc52f96b137.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nfranceschi2024explaining,\ntitle={Explaining Probabilistic Models with Distributional Values},\nauthor={Luca Franceschi and Michele Donini and Cedric Archambeau and Matthias Seeger},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=37xFIeYgE0}\n}"},"paperhash":{"value":"franceschi|explaining_probabilistic_models_with_distributional_values"}},"id":"37xFIeYgE0","forum":"37xFIeYgE0","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9644/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9644/Authors"],"number":9644,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9644/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873230162,"cdate":1706873230162,"tmdate":1719287294775,"mdate":1719287294775,"pdate":1714610499182,"odate":1717693103068,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering"},"authors":{"value":["Haoxuan Li","Chunyuan Zheng","Shuyi Wang","Kunhan Wu","Eric Wang","Peng Wu","Zhi Geng","Xu Chen","Xiao-Hua Zhou"]},"authorids":{"value":["~Haoxuan_Li6","~Chunyuan_Zheng1","~Shuyi_Wang3","~Kunhan_Wu1","~Eric_Wang3","~Peng_Wu5","~Zhi_Geng1","~Xu_Chen13","~Xiao-Hua_Zhou1"]},"abstract":{"value":"Recommender system aims to recommend items or information that may interest users based on their behaviors and preferences. However, there may be sampling selection bias in the data collection process, i.e., the collected data is not a representative of the target population. Many debiasing methods are developed based on pseudo-labelings. Nevertheless, the validity of these methods relies heavily on accurate pseudo-labelings (i.e., the imputed labels), which is difficult to satisfy in practice. In this paper, we theoretically propose several novel doubly robust estimators that are unbiased when either (a) the pseudo-labelings deviate from the true labels with an arbitrary user-specific inductive bias, item-specific inductive bias, or a combination of both, or (b) the learned propensities are accurate. We further propose a propensity reconstruction learning approach that adaptively updates the constraint weights using an attention mechanism and effectively controls the variance. Extensive experiments show that our approach outperforms the state-of-the-art on one semi-synthetic and three real-world datasets."},"pdf":{"value":"/pdf/09b15b7ff4236944230f4e2953820829ee23189e.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nli2024relaxing,\ntitle={Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering},\nauthor={Haoxuan Li and Chunyuan Zheng and Shuyi Wang and Kunhan Wu and Eric Wang and Peng Wu and Zhi Geng and Xu Chen and Xiao-Hua Zhou},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Ln3moCobjO}\n}"},"paperhash":{"value":"li|relaxing_the_accurate_imputation_assumption_in_doubly_robust_learning_for_debiased_collaborative_filtering"}},"id":"Ln3moCobjO","forum":"Ln3moCobjO","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9604/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9604/Authors"],"number":9604,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9604/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706873021527,"cdate":1706873021527,"tmdate":1719287294406,"mdate":1719287294406,"pdate":1714610498139,"odate":1717693102771,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Test-Time Degradation Adaptation for Open-Set Image Restoration"},"authors":{"value":["Yuanbiao Gou","Haiyu Zhao","Boyun Li","Xinyan Xiao","Xi Peng"]},"authorids":{"value":["~Yuanbiao_Gou1","~Haiyu_Zhao2","~Boyun_Li1","~Xinyan_Xiao1","~Xi_Peng3"]},"abstract":{"value":"In contrast to close-set scenarios that restore images from a predefined set of degradations, open-set image restoration aims to handle the unknown degradations that were unforeseen during the pretraining phase, which is less-touched as far as we know. This work study this challenging problem and reveal its essence as unidentified distribution shifts between the test and training data. Recently, test-time adaptation has emerged as a fundamental method to address this inherent disparities. Inspired by it, we propose a test-time degradation adaptation framework for open-set image restoration, which consists of three components, *i.e.*, i) a pre-trained and degradation-agnostic diffusion model for generating clean images, ii) a test-time degradation adapter adapts the unknown degradations based on the input image during the testing phase, and iii) the adapter-guided image restoration guides the model through the adapter to produce the corresponding clean image. Through experiments on multiple degradations, we show that our method achieves comparable even better performance than those task-specific methods. The code is available at https://github.com/XLearning-SCU/2024-ICML-TAO."},"pdf":{"value":"/pdf/cbe8a535cb6ad39d7f4315b6eaedd1bcc36a0a4d.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\ngou2024testtime,\ntitle={Test-Time Degradation Adaptation for Open-Set Image Restoration},\nauthor={Yuanbiao Gou and Haiyu Zhao and Boyun Li and Xinyan Xiao and Xi Peng},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=XLlQb24X2o}\n}"},"paperhash":{"value":"gou|testtime_degradation_adaptation_for_openset_image_restoration"}},"id":"XLlQb24X2o","forum":"XLlQb24X2o","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9594/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9594/Authors"],"number":9594,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9594/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706872969945,"cdate":1706872969945,"tmdate":1719287294333,"mdate":1719287294333,"pdate":1714610497986,"odate":1717693102707,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Regression with Multi-Expert Deferral"},"authors":{"value":["Anqi Mao","Mehryar Mohri","Yutao Zhong"]},"authorids":{"value":["~Anqi_Mao1","~Mehryar_Mohri2","~Yutao_Zhong1"]},"abstract":{"value":"Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of *regression with deferral*, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific. Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods. Our single-stage formulation subsumes as a special case the recent *regression with abstention* (Cheng et al., 2023) framework, where only a single expert is considered, specifically for the squared loss and a label-independent cost. Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral. We report the results of extensive experiments showing the effectiveness of our proposed algorithms."},"pdf":{"value":"/pdf/8bbe3b5d6cd9a25416cd56189ba8311bd6f2cd0d.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nmao2024regression,\ntitle={Regression with Multi-Expert Deferral},\nauthor={Anqi Mao and Mehryar Mohri and Yutao Zhong},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=5NTTCCO74S}\n}"},"paperhash":{"value":"mao|regression_with_multiexpert_deferral"}},"id":"5NTTCCO74S","forum":"5NTTCCO74S","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9593/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9593/Authors"],"number":9593,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9593/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706872959863,"cdate":1706872959863,"tmdate":1719287294320,"mdate":1719287294320,"pdate":1714610497939,"odate":1717693102687,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling"},"authors":{"value":["Yuxuan Yin","Yu Wang","Peng Li"]},"authorids":{"value":["~Yuxuan_Yin1","~Yu_Wang29","~Peng_Li8"]},"abstract":{"value":"We introduce a novel semi-supervised learning approach, named Teacher-Student Bayesian Optimization ($\\texttt{TSBO}$), integrating the teacher-student paradigm into BO to minimize expensive labeled data queries for the first time. $\\texttt{TSBO}$ incorporates a teacher model, an unlabeled data sampler, and a student model. The student is trained on unlabeled data locations generated by the sampler, with pseudo labels predicted by the teacher. The interplay between these three components implements a unique *selective regularization* to the teacher in the form of student feedback. This scheme enables the teacher to predict high-quality pseudo labels, enhancing the generalization of the GP surrogate model in the search space. To fully exploit $\\texttt{TSBO}$, we propose two optimized unlabeled data samplers to construct effective student feedback that well aligns with the objective of Bayesian optimization. Furthermore, we quantify and leverage the uncertainty of the teacher-student model for the provision of reliable feedback to the teacher in the presence of risky pseudo-label predictions. $\\texttt{TSBO}$ demonstrates significantly improved sample-efficiency in several global optimization tasks under tight labeled data budgets. The implementation is available at https://github.com/reminiscenty/TSBO-Official."},"pdf":{"value":"/pdf/93d979ee503e4fa5e5941f58a172226627bb4bd3.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nyin2024highdimensional,\ntitle={High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling},\nauthor={Yuxuan Yin and Yu Wang and Peng Li},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=beXQVQorse}\n}"},"paperhash":{"value":"yin|highdimensional_bayesian_optimization_via_semisupervised_learning_with_optimized_unlabeled_data_sampling"}},"id":"beXQVQorse","forum":"beXQVQorse","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9555/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9555/Authors"],"number":9555,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9555/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706872723455,"cdate":1706872723455,"tmdate":1719287294111,"mdate":1719287294111,"pdate":1714610497069,"odate":1717693102521,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models"},"authors":{"value":["Som Sagar","Aditya Taparia","Ransalu Senanayake"]},"authorids":{"value":["~Som_Sagar1","~Aditya_Taparia1","~Ransalu_Senanayake1"]},"abstract":{"value":"In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes *deep reinforcement learning* to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks."},"pdf":{"value":"/pdf/247b8e966933f650d3791001cd311c214f4ba1e0.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsagar2024failures,\ntitle={Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models},\nauthor={Som Sagar and Aditya Taparia and Ransalu Senanayake},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=DkqiId4AuR}\n}"},"paperhash":{"value":"sagar|failures_are_fated_but_can_be_faded_characterizing_and_mitigating_unwanted_behaviors_in_largescale_vision_and_language_models"}},"id":"DkqiId4AuR","forum":"DkqiId4AuR","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9539/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9539/Authors"],"number":9539,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9539/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706872596867,"cdate":1706872596867,"tmdate":1719287293922,"mdate":1719287293922,"pdate":1714610496686,"odate":1717693102331,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Efficient Pareto Manifold Learning with Low-Rank Structure"},"authors":{"value":["Weiyu Chen","James Kwok"]},"authorids":{"value":["~Weiyu_Chen1","~James_Kwok1"]},"abstract":{"value":"Multi-task learning, which optimizes performance across multiple tasks, is inherently a multi-objective optimization problem. Various algorithms are developed to provide discrete trade-off solutions on the Pareto front. Recently, continuous Pareto front approximations using a linear combination of base networks have emerged as a compelling strategy. However, it suffers from scalability issues when the number of tasks is large. To address this issue, we propose a novel approach that integrates a main network with several low-rank matrices to efficiently learn the Pareto manifold. It significantly reduces the number of parameters and facilitates the extraction of shared features. We also introduce orthogonal regularization to further bolster performance. Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines, especially on datasets with a large number of tasks."},"pdf":{"value":"/pdf/a1f08d9e0e6e7e084b0915dd6f5afcc164114055.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nchen2024efficient,\ntitle={Efficient Pareto Manifold Learning with Low-Rank Structure},\nauthor={Weiyu Chen and James Kwok},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=a2uFstsHPb}\n}"},"paperhash":{"value":"chen|efficient_pareto_manifold_learning_with_lowrank_structure"}},"id":"a2uFstsHPb","forum":"a2uFstsHPb","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9531/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9531/Authors"],"number":9531,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9531/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706872544022,"cdate":1706872544022,"tmdate":1719287293870,"mdate":1719287293870,"pdate":1714610496506,"odate":1717693102274,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images"},"authors":{"value":["Jun-Peng Jiang","Han-Jia Ye","Leye Wang","Yang Yang","Yuan Jiang","De-Chuan Zhan"]},"authorids":{"value":["~Jun-Peng_Jiang2","~Han-Jia_Ye1","~Leye_Wang1","~Yang_Yang17","~Yuan_Jiang1","~De-Chuan_Zhan1"]},"abstract":{"value":"Transferring knowledge across diverse data modalities is receiving increasing attention in machine learning. This paper tackles the task of leveraging expert-derived, yet expensive, tabular data to enhance image-based predictions when tabular data is unavailable during inference. The primary challenges stem from the inherent complexity of accurately mapping diverse tabular data to visual contexts, coupled with the necessity to devise distinct strategies for numerical and categorical tabular attributes. We propose CHannel tAbulaR alignment with optiMal tranSport (Charms), which establishes an alignment between image channels and tabular attributes, enabling selective knowledge transfer that is pertinent to visual features. Specifically, Charms measures similarity distributions across modalities to effectively differentiate and transfer relevant tabular features, with a focus on morphological characteristics, enhancing the capabilities of visual classifiers. By maximizing the mutual information between image channels and tabular features, knowledge from both numerical and categorical tabular attributes are extracted. Experimental results demonstrate that Charms not only enhances the performance of image classifiers but also improves their interpretability by effectively utilizing tabular knowledge."},"pdf":{"value":"/pdf/4d095d76cc72da640560b08e6139f176a8154083.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\njiang2024tabular,\ntitle={Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images},\nauthor={Jun-Peng Jiang and Han-Jia Ye and Leye Wang and Yang Yang and Yuan Jiang and De-Chuan Zhan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=v7I5FtL2pV}\n}"},"paperhash":{"value":"jiang|tabular_insights_visual_impacts_transferring_expertise_from_tables_to_images"}},"id":"v7I5FtL2pV","forum":"v7I5FtL2pV","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9447/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9447/Authors"],"number":9447,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9447/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706871930488,"cdate":1706871930488,"tmdate":1719287293404,"mdate":1719287293404,"pdate":1714610494428,"odate":1717693101795,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Convergence of Some Convex Message Passing Algorithms to a Fixed Point"},"authors":{"value":["Vaclav Voracek","Tomas Werner"]},"authorids":{"value":["~Vaclav_Voracek1","~Tomas_Werner1"]},"abstract":{"value":"A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. This is also known as convex/convergent message passing; examples are max-sum diffusion and sequential tree-reweighted message passing (TRW-S). Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any point). We prove a stronger result (conjectured before but never proved): the iterates converge to a fixed point of the method. Moreover, we show that the algorithm terminates within $\\mathcal{O}(1/\\varepsilon)$ iterations. We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective. Then we show that several convex message passing methods are special cases of this method. Finally, we show that a slightly different version of coordinate descent can cycle."},"pdf":{"value":"/pdf/c51ff165c9706dbb542a753de9f0ec70ee0a2555.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nvoracek2024convergence,\ntitle={Convergence of Some Convex Message Passing Algorithms to a Fixed Point},\nauthor={Vaclav Voracek and Tomas Werner},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=CaxQ5IbHgF}\n}"},"paperhash":{"value":"voracek|convergence_of_some_convex_message_passing_algorithms_to_a_fixed_point"}},"id":"CaxQ5IbHgF","forum":"CaxQ5IbHgF","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9330/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9330/Authors"],"number":9330,"pdate":1714610491249,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9330/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706871158419,"cdate":1706871158419,"tmdate":1719287292242,"mdate":1719287292242,"odate":1717693100882,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"},"authors":{"value":["Kevin Frans","Seohong Park","Pieter Abbeel","Sergey Levine"]},"authorids":{"value":["~Kevin_Frans1","~Seohong_Park1","~Pieter_Abbeel2","~Sergey_Levine1"]},"abstract":{"value":"Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a *functional* reward encoding (FRE) as a general, scalable solution to this *zero-shot RL* problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods."},"pdf":{"value":"/pdf/d3b3b9aac1b31718d454799ac770e55071f1bd77.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nfrans2024unsupervised,\ntitle={Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings},\nauthor={Kevin Frans and Seohong Park and Pieter Abbeel and Sergey Levine},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=a6wCNfIj8E}\n}"},"paperhash":{"value":"frans|unsupervised_zeroshot_reinforcement_learning_via_functional_reward_encodings"}},"id":"a6wCNfIj8E","forum":"a6wCNfIj8E","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9161/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9161/Authors"],"number":9161,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9161/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706869511954,"cdate":1706869511954,"tmdate":1719287290957,"mdate":1719287290957,"pdate":1714610486801,"odate":1717693099715,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Neural Jump-Diffusion Temporal Point Processes"},"authors":{"value":["Shuai Zhang","Chuan Zhou","Yang Aron Liu","Peng Zhang","Xixun Lin","Zhi-Ming Ma"]},"authorids":{"value":["~Shuai_Zhang23","~Chuan_Zhou3","~Yang_Aron_Liu1","~Peng_Zhang55","~Xixun_Lin3","~Zhi-Ming_Ma1"]},"abstract":{"value":"We present a novel perspective on temporal point processes (TPPs) by reformulating their intensity processes as solutions to stochastic differential equations (SDEs). In particular, we first prove the equivalent SDE formulations of several classical TPPs, including Poisson processes, Hawkes processes, and self-correcting processes. Based on these proofs, we introduce a unified TPP framework called Neural Jump-Diffusion Temporal Point Process (NJDTPP), whose intensity process is governed by a neural jump-diffusion SDE (NJDSDE) where the drift, diffusion, and jump coefficient functions are parameterized by neural networks. Compared to previous works, NJDTPP exhibits model flexibility in capturing intensity dynamics without relying on any specific functional form, and provides theoretical guarantees regarding the existence and uniqueness of the solution to the proposed NJDSDE. Experiments on both synthetic and real-world datasets demonstrate that NJDTPP is capable of capturing the dynamics of intensity processes in different scenarios and significantly outperforms the state-of-the-art TPP models in prediction tasks."},"pdf":{"value":"/pdf/1345e93f422f204c2bc1ae968981d0ea53c11d40.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nzhang2024neural,\ntitle={Neural Jump-Diffusion Temporal Point Processes},\nauthor={Shuai Zhang and Chuan Zhou and Yang Aron Liu and Peng Zhang and Xixun Lin and Zhi-Ming Ma},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=d1P6GtRzuV}\n}"},"paperhash":{"value":"zhang|neural_jumpdiffusion_temporal_point_processes"}},"id":"d1P6GtRzuV","forum":"d1P6GtRzuV","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9108/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9108/Authors"],"number":9108,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9108/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706868963505,"cdate":1706868963505,"tmdate":1722630821843,"mdate":1722630821843,"pdate":1714610485516,"odate":1717693099331,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models"},"authors":{"value":["Louis Sharrock","Jack Simons","Song Liu","Mark Beaumont"]},"authorids":{"value":["~Louis_Sharrock1","~Jack_Simons1","~Song_Liu1","~Mark_Beaumont1"]},"abstract":{"value":"We introduce Sequential Neural Posterior Score Estimation (SNPSE), a score-based method for Bayesian inference in simulator-based models. Our method, inspired by the remarkable success of score-based methods in generative modelling, leverages conditional score-based diffusion models to generate samples from the posterior distribution of interest. The model is trained using an objective function which directly estimates the score of the posterior. We embed the model into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We also introduce several alternative sequential approaches, and discuss their relative merits. We then validate our method, as well as its amortised, non-sequential, variant on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE)."},"pdf":{"value":"/pdf/4c9e7d18fee6984c5878c0a23abcef3430988813.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsharrock2024sequential,\ntitle={Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models},\nauthor={Louis Sharrock and Jack Simons and Song Liu and Mark Beaumont},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=8viuf9PdzU}\n}"},"paperhash":{"value":"sharrock|sequential_neural_score_estimation_likelihoodfree_inference_with_conditional_score_based_diffusion_models"}},"id":"8viuf9PdzU","forum":"8viuf9PdzU","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission9103/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission9103/Authors"],"number":9103,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission9103/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706868857793,"cdate":1706868857793,"tmdate":1719287290240,"mdate":1719287290240,"pdate":1714610485443,"odate":1717693099274,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Model Alignment as Prospect Theoretic Optimization"},"authors":{"value":["Kawin Ethayarajh","Winnie Xu","Niklas Muennighoff","Dan Jurafsky","Douwe Kiela"]},"authorids":{"value":["~Kawin_Ethayarajh1","~Winnie_Xu1","~Niklas_Muennighoff1","~Dan_Jurafsky1","~Douwe_Kiela1"]},"abstract":{"value":"Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration."},"pdf":{"value":"/pdf/07f780dd14f712d2c4d4ec8abf5ef019f0f8889f.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nethayarajh2024model,\ntitle={Model Alignment as Prospect Theoretic Optimization},\nauthor={Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=iUwHnoENnl}\n}"},"paperhash":{"value":"ethayarajh|model_alignment_as_prospect_theoretic_optimization"}},"id":"iUwHnoENnl","forum":"iUwHnoENnl","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8860/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8860/Authors"],"number":8860,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8860/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706866567373,"cdate":1706866567373,"tmdate":1719287287794,"mdate":1719287287794,"pdate":1714610479411,"odate":1717693097450,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Stay on Topic with Classifier-Free Guidance"},"authors":{"value":["Guillaume Sanchez","Alexander Spangher","Honglu Fan","Elad Levi","Stella Biderman"]},"authorids":{"value":["~Guillaume_Sanchez1","~Alexander_Spangher2","~Honglu_Fan1","~Elad_Levi1","~Stella_Biderman1"]},"abstract":{"value":"Classifier-Free Guidance (CFG) has recently emerged in as a lightweight technique to encourage prompt-adherence in generations, yet has not yet been successfully applied to language modeling. In this work, we demonstrate across a wide array of benchmarks that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across: Q&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in human evaluations we show a 75% preference for using CFG over baseline."},"pdf":{"value":"/pdf/d56ce0ef08d1fd494f94ff5cf8fca814f9bda28f.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsanchez2024stay,\ntitle={Stay on Topic with Classifier-Free Guidance},\nauthor={Guillaume Sanchez and Alexander Spangher and Honglu Fan and Elad Levi and Stella Biderman},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=RiM3cl9MdK}\n}"},"paperhash":{"value":"sanchez|stay_on_topic_with_classifierfree_guidance"}},"id":"RiM3cl9MdK","forum":"RiM3cl9MdK","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8828/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8828/Authors"],"number":8828,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8828/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706866137817,"cdate":1706866137817,"tmdate":1719287287485,"mdate":1719287287485,"pdate":1714610478704,"odate":1717693097166,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"},"authors":{"value":["Haocheng Xi","Yuxiang Chen","Kang Zhao","KAI JUN TEH","Jianfei Chen","Jun Zhu"]},"authorids":{"value":["~Haocheng_Xi1","~Yuxiang_Chen2","~Kang_Zhao5","~KAI_JUN_TEH1","~Jianfei_Chen1","~Jun_Zhu2"]},"abstract":{"value":"Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline."},"pdf":{"value":"/pdf/4cc62e8dd2f029713ae4205494b3828fe241608c.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nxi2024jetfire,\ntitle={Jetfire: Efficient and Accurate Transformer Pretraining with {INT}8 Data Flow and Per-Block Quantization},\nauthor={Haocheng Xi and Yuxiang Chen and Kang Zhao and KAI JUN TEH and Jianfei Chen and Jun Zhu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ltzTHGFF5i}\n}"},"paperhash":{"value":"xi|jetfire_efficient_and_accurate_transformer_pretraining_with_int8_data_flow_and_perblock_quantization"}},"id":"ltzTHGFF5i","forum":"ltzTHGFF5i","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8752/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8752/Authors"],"number":8752,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8752/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706865165877,"cdate":1706865165877,"tmdate":1719287286887,"mdate":1719287286887,"pdate":1714610477016,"odate":1717693096540,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"QuRating: Selecting High-Quality Data for Training Language Models"},"authors":{"value":["Alexander Wettig","Aatmik Gupta","Saumya Malik","Danqi Chen"]},"authorids":{"value":["~Alexander_Wettig1","~Aatmik_Gupta1","~Saumya_Malik1","~Danqi_Chen1"]},"abstract":{"value":"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."},"pdf":{"value":"/pdf/bf04a0460b17177a766e67a1c2e65af4b785f1ab.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nwettig2024qurating,\ntitle={QuRating: Selecting High-Quality Data for Training Language Models},\nauthor={Alexander Wettig and Aatmik Gupta and Saumya Malik and Danqi Chen},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=GLGYYqPwjy}\n}"},"paperhash":{"value":"wettig|qurating_selecting_highquality_data_for_training_language_models"}},"id":"GLGYYqPwjy","forum":"GLGYYqPwjy","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8695/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8695/Authors"],"number":8695,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8695/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706864461961,"cdate":1706864461961,"tmdate":1719287286331,"mdate":1719287286331,"pdate":1714610475638,"odate":1717693095989,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Tuning-Free Stochastic Optimization"},"authors":{"value":["Ahmed Khaled","Chi Jin"]},"authorids":{"value":["~Ahmed_Khaled1","~Chi_Jin1"]},"abstract":{"value":"Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of *``tuning-free''* algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability."},"pdf":{"value":"/pdf/289368853b81ca3dd675daa44706212cb51afcb0.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nkhaled2024tuningfree,\ntitle={Tuning-Free Stochastic Optimization},\nauthor={Ahmed Khaled and Chi Jin},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=A6fmX9QCEa}\n}"},"paperhash":{"value":"khaled|tuningfree_stochastic_optimization"}},"id":"A6fmX9QCEa","forum":"A6fmX9QCEa","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8527/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8527/Authors"],"number":8527,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8527/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706862586975,"cdate":1706862586975,"tmdate":1719287284356,"mdate":1719287284356,"pdate":1714610471557,"odate":1717693094543,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Second-Order Uncertainty Quantification: A Distance-Based Approach"},"authors":{"value":["Yusuf Sale","Viktor Bengs","Michele Caprio","Eyke Hüllermeier"]},"authorids":{"value":["~Yusuf_Sale1","~Viktor_Bengs1","~Michele_Caprio1","~Eyke_Hüllermeier1"]},"abstract":{"value":"In the past couple of years, various approaches to representing and quantifying different types of predictive uncertainty in machine learning, notably in the setting of classification, have been proposed on the basis of second-order probability distributions, i.e., predictions in the form of distributions on probability distributions. A completely conclusive solution has not yet been found, however, as shown by recent criticisms of commonly used uncertainty measures associated with second-order distributions, identifying undesirable theoretical properties of these measures. In light of these criticisms, we propose a set of formal criteria that meaningful uncertainty measures for predictive uncertainty based on second-order distributions should obey. Moreover, we provide a general framework for developing uncertainty measures to account for these criteria, and offer an instantiation based on the Wasserstein distance, for which we prove that all criteria are satisfied."},"pdf":{"value":"/pdf/cfa1b1d90c327404b4ec994176ee0fb090e539f1.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsale2024secondorder,\ntitle={Second-Order Uncertainty Quantification: A Distance-Based Approach},\nauthor={Yusuf Sale and Viktor Bengs and Michele Caprio and Eyke H{\\\"u}llermeier},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=VJjjNrUi8j}\n}"},"paperhash":{"value":"sale|secondorder_uncertainty_quantification_a_distancebased_approach"}},"id":"VJjjNrUi8j","forum":"VJjjNrUi8j","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8522/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8522/Authors"],"number":8522,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8522/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706862546879,"cdate":1706862546879,"tmdate":1719287284328,"mdate":1719287284328,"pdate":1714610471452,"odate":1717693094482,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Fundamental Benefit of Alternating Updates in Minimax Optimization"},"authors":{"value":["Jaewook Lee","Hanseul Cho","Chulhee Yun"]},"authorids":{"value":["~Jaewook_Lee6","~Hanseul_Cho1","~Chulhee_Yun1"]},"abstract":{"value":"The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller iteration complexity bound, identical to that of the Extra-gradient method, while requiring less gradient computations. We also prove that Alex-GDA enjoys linear convergence for bilinear problems, for which both Sim-GDA and Alt-GDA fail to converge at all."},"pdf":{"value":"/pdf/4956074a8c819accae3517466ddf6bd654e6d965.pdf"},"venue":{"value":"ICML 2024 Spotlight"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nlee2024fundamental,\ntitle={Fundamental Benefit of Alternating Updates in Minimax Optimization},\nauthor={Jaewook Lee and Hanseul Cho and Chulhee Yun},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=s6ZAT8MLKU}\n}"},"paperhash":{"value":"lee|fundamental_benefit_of_alternating_updates_in_minimax_optimization"}},"id":"s6ZAT8MLKU","forum":"s6ZAT8MLKU","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission8387/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission8387/Authors"],"number":8387,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission8387/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706860482316,"cdate":1706860482316,"tmdate":1719287283039,"mdate":1719287283039,"pdate":1714610467997,"odate":1717693093221,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}}],"count":191,"fromCache":true}