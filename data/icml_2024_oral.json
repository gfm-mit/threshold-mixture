{
  "notes": [
    {
      "content": {
        "title": {
          "value": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo"
        },
        "authors": {
          "value": [
            "Stephen Zhao",
            "Rob Brekelmans",
            "Alireza Makhzani",
            "Roger Baker Grosse"
          ]
        },
        "authorids": {
          "value": [
            "~Stephen_Zhao1",
            "~Rob_Brekelmans1",
            "~Alireza_Makhzani1",
            "~Roger_Baker_Grosse1"
          ]
        },
        "abstract": {
          "value": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks."
        },
        "pdf": {
          "value": "/pdf/eada04b50249701256b116303609e5684dc2beff.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nzhao2024probabilistic,\ntitle={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo},\nauthor={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Baker Grosse},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=frA0NNBS1n}\n}"
        },
        "paperhash": {
          "value": "zhao|probabilistic_inference_in_language_models_via_twisted_sequential_monte_carlo"
        }
      },
      "id": "frA0NNBS1n",
      "forum": "frA0NNBS1n",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission10076/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission10076/Authors"
      ],
      "number": 10076,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission10076/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706874982624,
      "cdate": 1706874982624,
      "tmdate": 1719287297435,
      "mdate": 1719287297435,
      "pdate": 1714610509454,
      "odate": 1717693105907,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence"
        },
        "authors": {
          "value": [
            "Edward Hughes",
            "Michael D Dennis",
            "Jack Parker-Holder",
            "Feryal Behbahani",
            "Aditi Mavalankar",
            "Yuge Shi",
            "Tom Schaul",
            "Tim Rockt\u00e4schel"
          ]
        },
        "authorids": {
          "value": [
            "~Edward_Hughes1",
            "~Michael_D_Dennis1",
            "~Jack_Parker-Holder1",
            "~Feryal_Behbahani1",
            "~Aditi_Mavalankar1",
            "~Yuge_Shi2",
            "~Tom_Schaul2",
            "~Tim_Rockt\u00e4schel1"
          ]
        },
        "abstract": {
          "value": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future."
        },
        "pdf": {
          "value": "/pdf/f1c451838b8e49a9d9e79b3c5f89a0b73098694b.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nhughes2024position,\ntitle={Position: Open-Endedness is Essential for Artificial Superhuman Intelligence},\nauthor={Edward Hughes and Michael D Dennis and Jack Parker-Holder and Feryal Behbahani and Aditi Mavalankar and Yuge Shi and Tom Schaul and Tim Rockt{\\\"a}schel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Bc4vZ2CX7E}\n}"
        },
        "paperhash": {
          "value": "hughes|position_openendedness_is_essential_for_artificial_superhuman_intelligence"
        }
      },
      "id": "Bc4vZ2CX7E",
      "forum": "Bc4vZ2CX7E",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9943/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9943/Authors"
      ],
      "number": 9943,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9943/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706874563451,
      "cdate": 1706874563451,
      "tmdate": 1719287296767,
      "mdate": 1719287296767,
      "pdate": 1714610506405,
      "odate": 1717693105189,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL"
        },
        "authors": {
          "value": [
            "Jesse Farebrother",
            "Jordi Orbay",
            "Quan Vuong",
            "Adrien Ali Taiga",
            "Yevgen Chebotar",
            "Ted Xiao",
            "Alex Irpan",
            "Sergey Levine",
            "Pablo Samuel Castro",
            "Aleksandra Faust",
            "Aviral Kumar",
            "Rishabh Agarwal"
          ]
        },
        "authorids": {
          "value": [
            "~Jesse_Farebrother1",
            "~Jordi_Orbay1",
            "~Quan_Vuong2",
            "~Adrien_Ali_Taiga1",
            "~Yevgen_Chebotar1",
            "~Ted_Xiao1",
            "~Alex_Irpan1",
            "~Sergey_Levine1",
            "~Pablo_Samuel_Castro1",
            "~Aleksandra_Faust1",
            "~Aviral_Kumar2",
            "~Rishabh_Agarwal2"
          ]
        },
        "abstract": {
          "value": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost."
        },
        "pdf": {
          "value": "/pdf/d7be2bfd406a29a0a8152605d8f88ab08afd5f3d.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nfarebrother2024stop,\ntitle={Stop Regressing: Training Value Functions via Classification for Scalable Deep {RL}},\nauthor={Jesse Farebrother and Jordi Orbay and Quan Vuong and Adrien Ali Taiga and Yevgen Chebotar and Ted Xiao and Alex Irpan and Sergey Levine and Pablo Samuel Castro and Aleksandra Faust and Aviral Kumar and Rishabh Agarwal},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dVpFKfqF3R}\n}"
        },
        "paperhash": {
          "value": "farebrother|stop_regressing_training_value_functions_via_classification_for_scalable_deep_rl"
        }
      },
      "id": "dVpFKfqF3R",
      "forum": "dVpFKfqF3R",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9864/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9864/Authors"
      ],
      "number": 9864,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9864/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706874259638,
      "cdate": 1706874259638,
      "tmdate": 1719287296173,
      "mdate": 1719287296173,
      "pdate": 1714610504513,
      "odate": 1717693104542,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Improving Transformers with Dynamically Composable Multi-Head Attention"
        },
        "authors": {
          "value": [
            "Da Xiao",
            "Qingye Meng",
            "Shengping Li",
            "xingyuan yuan"
          ]
        },
        "authorids": {
          "value": [
            "~Da_Xiao1",
            "~Qingye_Meng1",
            "~Shengping_Li2",
            "~xingyuan_yuan1"
          ]
        },
        "abstract": {
          "value": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a Compose function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with 1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation."
        },
        "pdf": {
          "value": "/pdf/ff4c9fbb27e8d39271c90edee9bc164c464d96c9.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nxiao2024improving,\ntitle={Improving Transformers with Dynamically Composable Multi-Head Attention},\nauthor={Da Xiao and Qingye Meng and Shengping Li and xingyuan yuan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=RbiBKPtuHp}\n}"
        },
        "paperhash": {
          "value": "xiao|improving_transformers_with_dynamically_composable_multihead_attention"
        }
      },
      "id": "RbiBKPtuHp",
      "forum": "RbiBKPtuHp",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9473/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9473/Authors"
      ],
      "number": 9473,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9473/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706872134939,
      "cdate": 1706872134939,
      "tmdate": 1719287293565,
      "mdate": 1719287293565,
      "pdate": 1714610495076,
      "odate": 1717693101988,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Learning Useful Representations of Recurrent Neural Network Weight Matrices"
        },
        "authors": {
          "value": [
            "Vincent Herrmann",
            "Francesco Faccio",
            "J\u00fcrgen Schmidhuber"
          ]
        },
        "authorids": {
          "value": [
            "~Vincent_Herrmann1",
            "~Francesco_Faccio1",
            "~J\u00fcrgen_Schmidhuber1"
          ]
        },
        "abstract": {
          "value": "Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the _mechanistic approach_ directly looks at some RNN's weights to predict its behavior, the _functionalist approach_ analyzes its overall functionality\u2013specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority."
        },
        "pdf": {
          "value": "/pdf/abdd59b9022e26fa87966eb021c1a9debc4b9662.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nherrmann2024learning,\ntitle={Learning Useful Representations of Recurrent Neural Network Weight Matrices},\nauthor={Vincent Herrmann and Francesco Faccio and J{\\\"u}rgen Schmidhuber},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=QBj7Uurdwf}\n}"
        },
        "paperhash": {
          "value": "herrmann|learning_useful_representations_of_recurrent_neural_network_weight_matrices"
        }
      },
      "id": "QBj7Uurdwf",
      "forum": "QBj7Uurdwf",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9340/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9340/Authors"
      ],
      "number": 9340,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9340/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706871206652,
      "cdate": 1706871206652,
      "tmdate": 1719287292277,
      "mdate": 1719287292277,
      "pdate": 1714610491498,
      "odate": 1717693100898,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model"
        },
        "authors": {
          "value": [
            "Fei Liu",
            "Tong Xialiang",
            "Mingxuan Yuan",
            "Xi Lin",
            "Fu Luo",
            "Zhenkun Wang",
            "Zhichao Lu",
            "Qingfu Zhang"
          ]
        },
        "authorids": {
          "value": [
            "~Fei_Liu14",
            "~Tong_Xialiang2",
            "~Mingxuan_Yuan1",
            "~Xi_Lin2",
            "~Fu_Luo1",
            "~Zhenkun_Wang1",
            "~Zhichao_Lu1",
            "~Qingfu_Zhang1"
          ]
        },
        "abstract": {
          "value": "Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often very labour extensive and requires rich working experience and knowledge. This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem."
        },
        "pdf": {
          "value": "/pdf/04b3b8dd683352d46b67cde2cef119f3f319ca94.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nliu2024evolution,\ntitle={Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model},\nauthor={Fei Liu and Tong Xialiang and Mingxuan Yuan and Xi Lin and Fu Luo and Zhenkun Wang and Zhichao Lu and Qingfu Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=BwAkaxqiLB}\n}"
        },
        "paperhash": {
          "value": "liu|evolution_of_heuristics_towards_efficient_automatic_algorithm_design_using_large_language_model"
        }
      },
      "id": "BwAkaxqiLB",
      "forum": "BwAkaxqiLB",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9269/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9269/Authors"
      ],
      "number": 9269,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9269/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706870593519,
      "cdate": 1706870593519,
      "tmdate": 1719287291806,
      "mdate": 1719287291806,
      "pdate": 1714610489612,
      "odate": 1717693100524,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code"
        },
        "authors": {
          "value": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Aashi Jain",
            "Thomas Kipf",
            "Yisong Yue",
            "David A Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
          ]
        },
        "authorids": {
          "value": [
            "~Ziniu_Hu1",
            "~Ahmet_Iscen3",
            "~Aashi_Jain1",
            "~Thomas_Kipf2",
            "~Yisong_Yue1",
            "~David_A_Ross1",
            "~Cordelia_Schmid1",
            "~Alireza_Fathi1"
          ]
        },
        "abstract": {
          "value": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal."
        },
        "pdf": {
          "value": "/pdf/d85fff2e858b46f982dd07918337607cfe370cc6.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nhu2024scenecraft,\ntitle={SceneCraft: An {LLM} Agent for Synthesizing 3D Scenes as Blender Code},\nauthor={Ziniu Hu and Ahmet Iscen and Aashi Jain and Thomas Kipf and Yisong Yue and David A Ross and Cordelia Schmid and Alireza Fathi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=gAyzjHw2ml}\n}"
        },
        "paperhash": {
          "value": "hu|scenecraft_an_llm_agent_for_synthesizing_3d_scenes_as_blender_code"
        }
      },
      "id": "gAyzjHw2ml",
      "forum": "gAyzjHw2ml",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9223/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9223/Authors"
      ],
      "number": 9223,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9223/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706870158791,
      "cdate": 1706870158791,
      "tmdate": 1719287291393,
      "mdate": 1719287291393,
      "pdate": 1714610488315,
      "odate": 1717693100159,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning"
        },
        "authors": {
          "value": [
            "Weilin Chen",
            "Ruichu Cai",
            "Zeqin Yang",
            "Jie Qiao",
            "Yuguang Yan",
            "Zijian Li",
            "Zhifeng Hao"
          ]
        },
        "authorids": {
          "value": [
            "~Weilin_Chen1",
            "~Ruichu_Cai1",
            "~Zeqin_Yang1",
            "~Jie_Qiao1",
            "~Yuguang_Yan1",
            "~Zijian_Li1",
            "~Zhifeng_Hao5"
          ]
        },
        "abstract": {
          "value": "Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators."
        },
        "pdf": {
          "value": "/pdf/fae46b2cf3e635e7d98f920bdce60cc0a2230b6a.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nchen2024doubly,\ntitle={Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning},\nauthor={Weilin Chen and Ruichu Cai and Zeqin Yang and Jie Qiao and Yuguang Yan and Zijian Li and Zhifeng Hao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=5lI9wm4dws}\n}"
        },
        "paperhash": {
          "value": "chen|doubly_robust_causal_effect_estimation_under_networked_interference_via_targeted_learning"
        }
      },
      "id": "5lI9wm4dws",
      "forum": "5lI9wm4dws",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9165/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9165/Authors"
      ],
      "number": 9165,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9165/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706869541327,
      "cdate": 1706869541327,
      "tmdate": 1719287290985,
      "mdate": 1719287290985,
      "pdate": 1714610486906,
      "odate": 1717693099735,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Emergent Equivariance in Deep Ensembles"
        },
        "authors": {
          "value": [
            "Jan E Gerken",
            "Pan Kessel"
          ]
        },
        "authorids": {
          "value": [
            "~Jan_E_Gerken1",
            "~Pan_Kessel1"
          ]
        },
        "abstract": {
          "value": "We show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments."
        },
        "pdf": {
          "value": "/pdf/3cfee8dbeb79d0989c8616f4a93148cb89e4b783.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\ngerken2024emergent,\ntitle={Emergent Equivariance in Deep Ensembles},\nauthor={Jan E Gerken and Pan Kessel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=plXXbXjvQ9}\n}"
        },
        "paperhash": {
          "value": "gerken|emergent_equivariance_in_deep_ensembles"
        }
      },
      "id": "plXXbXjvQ9",
      "forum": "plXXbXjvQ9",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission9110/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission9110/Authors"
      ],
      "number": 9110,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission9110/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706868971924,
      "cdate": 1706868971924,
      "tmdate": 1719287290410,
      "mdate": 1719287290410,
      "pdate": 1714610485556,
      "odate": 1717693099360,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks"
        },
        "authors": {
          "value": [
            "Linyuan Gong",
            "Sida Wang",
            "Mostafa Elhoushi",
            "Alvin Cheung"
          ]
        },
        "authorids": {
          "value": [
            "~Linyuan_Gong1",
            "~Sida_Wang2",
            "~Mostafa_Elhoushi1",
            "~Alvin_Cheung2"
          ]
        },
        "abstract": {
          "value": "We introduce **S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com."
        },
        "pdf": {
          "value": "/pdf/756353f664b16cd35a013ee9dd2a583f6b18bc9b.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\ngong2024evaluation,\ntitle={Evaluation of {LLM}s on Syntax-Aware Code Fill-in-the-Middle Tasks},\nauthor={Linyuan Gong and Sida Wang and Mostafa Elhoushi and Alvin Cheung},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jKYyFbH8ap}\n}"
        },
        "paperhash": {
          "value": "gong|evaluation_of_llms_on_syntaxaware_code_fillinthemiddle_tasks"
        }
      },
      "id": "jKYyFbH8ap",
      "forum": "jKYyFbH8ap",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8983/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8983/Authors"
      ],
      "number": 8983,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8983/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706867715263,
      "cdate": 1706867715263,
      "tmdate": 1719287288727,
      "mdate": 1719287288727,
      "pdate": 1714610482546,
      "odate": 1717693098306,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Position: Automatic Environment Shaping is the Next Frontier in RL"
        },
        "authors": {
          "value": [
            "Younghyo Park",
            "Gabriel B. Margolis",
            "Pulkit Agrawal"
          ]
        },
        "authorids": {
          "value": [
            "~Younghyo_Park1",
            "~Gabriel_B._Margolis1",
            "~Pulkit_Agrawal1"
          ]
        },
        "abstract": {
          "value": "Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures."
        },
        "pdf": {
          "value": "/pdf/ab986a7984eac1598beea86386428fa53f5694f3.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\npark2024position,\ntitle={Position: Automatic Environment Shaping is the Next Frontier in {RL}},\nauthor={Younghyo Park and Gabriel B. Margolis and Pulkit Agrawal},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dslUyy1rN4}\n}"
        },
        "paperhash": {
          "value": "park|position_automatic_environment_shaping_is_the_next_frontier_in_rl"
        }
      },
      "id": "dslUyy1rN4",
      "forum": "dslUyy1rN4",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8768/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8768/Authors"
      ],
      "number": 8768,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8768/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706865428583,
      "cdate": 1706865428583,
      "tmdate": 1719287287050,
      "mdate": 1719287287050,
      "pdate": 1714610477467,
      "odate": 1717693096706,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning"
        },
        "authors": {
          "value": [
            "Qiankun Zhang",
            "Aocheng Shen",
            "Boyu Zhang",
            "Hanrui Jiang",
            "Bingqian Du"
          ]
        },
        "authorids": {
          "value": [
            "~Qiankun_Zhang1",
            "~Aocheng_Shen1",
            "~Boyu_Zhang6",
            "~Hanrui_Jiang1",
            "~Bingqian_Du1"
          ]
        },
        "abstract": {
          "value": "For a specific online optimization problem, for example, online bipartite matching (OBM), research efforts could be made in two directions before it is finally closed, i.e., the optimal competitive online algorithm is found. One is to continuously design algorithms with better performance. To this end, reinforcement learning (RL) has demonstrated great success in literature. However, little is known on the other direction: whether RL helps explore how hard an online problem is. In this paper, we study a generalized model of OBM, named online matching with stochastic rewards (OMSR, FOCS 2012), for which the optimal competitive ratio is still unknown. We adopt an adversarial RL approach that trains two RL agents adversarially and iteratively: the algorithm agent learns for algorithms with larger competitive ratios, while the adversarial agent learns to produce a family of hard instances. Through such a framework, agents converge at the end with a robust algorithm, which empirically outperforms the state of the art (STOC 2020). Much more significantly, it allows to track how the hard instances are generated. We succeed in distilling two structural properties from the learned graph patterns, which remarkably reduce the action space, and further enable theoretical improvement on the best-known hardness result of OMSR, from $0.621$ (FOCS 2012) to $0.597$. To the best of our knowledge, this gives the first evidence that RL can help enhance the theoretical understanding of an online problem."
        },
        "pdf": {
          "value": "/pdf/b80295721b6d43dca0fd67f3b8e93fd7d8c608a2.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nzhang2024online,\ntitle={Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning},\nauthor={Qiankun Zhang and Aocheng Shen and Boyu Zhang and Hanrui Jiang and Bingqian Du},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=TujtZgdRxB}\n}"
        },
        "paperhash": {
          "value": "zhang|online_matching_with_stochastic_rewards_provable_better_bound_via_adversarial_reinforcement_learning"
        }
      },
      "id": "TujtZgdRxB",
      "forum": "TujtZgdRxB",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8765/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8765/Authors"
      ],
      "number": 8765,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8765/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706865396454,
      "cdate": 1706865396454,
      "tmdate": 1719287287029,
      "mdate": 1719287287029,
      "pdate": 1714610477371,
      "odate": 1717693096669,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis"
        },
        "authors": {
          "value": [
            "Jessica Dai"
          ]
        },
        "authorids": {
          "value": [
            "~Jessica_Dai1"
          ]
        },
        "abstract": {
          "value": "What is *agency,* and why does it matter? In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent. The first view, which we term *mechanistic*, is commonly\u2014 and implicitly\u2014assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI. Under the second view, which we term volitional, AI can no longer be considered an ethical agent. We discuss the implications of each of these views for two critical questions: first, what the ideal system \u201cought\u201d to look like, and second, how accountability may be achieved. In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes."
        },
        "pdf": {
          "value": "/pdf/5c25926a15ae66c549816cb208b911d880e03eeb.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\ndai2024position,\ntitle={Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis},\nauthor={Jessica Dai},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=4XlGXIh2BB}\n}"
        },
        "paperhash": {
          "value": "dai|position_beyond_personhood_agency_accountability_and_the_limits_of_anthropomorphic_ethical_analysis"
        }
      },
      "id": "4XlGXIh2BB",
      "forum": "4XlGXIh2BB",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8735/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8735/Authors"
      ],
      "number": 8735,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8735/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706864959609,
      "cdate": 1706864959609,
      "tmdate": 1719287286751,
      "mdate": 1719287286751,
      "pdate": 1714610476580,
      "odate": 1717693096364,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape"
        },
        "authors": {
          "value": [
            "Juno Kim",
            "Taiji Suzuki"
          ]
        },
        "authorids": {
          "value": [
            "~Juno_Kim1",
            "~Taiji_Suzuki1"
          ]
        },
        "abstract": {
          "value": "Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest."
        },
        "pdf": {
          "value": "/pdf/9f326608e738bb5a2b10f0c281623010a0fc3955.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nkim2024transformers,\ntitle={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},\nauthor={Juno Kim and Taiji Suzuki},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=xm2lU7tteQ}\n}"
        },
        "paperhash": {
          "value": "kim|transformers_learn_nonlinear_features_in_context_nonconvex_meanfield_dynamics_on_the_attention_landscape"
        }
      },
      "id": "xm2lU7tteQ",
      "forum": "xm2lU7tteQ",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8728/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8728/Authors"
      ],
      "number": 8728,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8728/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706864891677,
      "cdate": 1706864891677,
      "tmdate": 1719287286667,
      "mdate": 1719287286667,
      "pdate": 1714610476407,
      "odate": 1717693096288,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews"
        },
        "authors": {
          "value": [
            "Weixin Liang",
            "Zachary Izzo",
            "Yaohui Zhang",
            "Haley Lepp",
            "Hancheng Cao",
            "Xuandong Zhao",
            "Lingjiao Chen",
            "Haotian Ye",
            "Sheng Liu",
            "Zhi Huang",
            "Daniel McFarland",
            "James Y. Zou"
          ]
        },
        "authorids": {
          "value": [
            "~Weixin_Liang1",
            "~Zachary_Izzo1",
            "~Yaohui_Zhang2",
            "~Haley_Lepp1",
            "~Hancheng_Cao1",
            "~Xuandong_Zhao1",
            "~Lingjiao_Chen1",
            "~Haotian_Ye1",
            "~Sheng_Liu2",
            "zhihuang@stanford.edu",
            "~Daniel_McFarland1",
            "~James_Y._Zou1"
          ]
        },
        "abstract": {
          "value": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: *ICLR* 2024, *NeurIPS* 2023, *CoRL* 2023 and *EMNLP* 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."
        },
        "pdf": {
          "value": "/pdf/387754028a844300fa28d67c2c2d9ac7a8ca3182.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nliang2024monitoring,\ntitle={Monitoring {AI}-Modified Content at Scale: A Case Study on the Impact of Chat{GPT} on {AI} Conference Peer Reviews},\nauthor={Weixin Liang and Zachary Izzo and Yaohui Zhang and Haley Lepp and Hancheng Cao and Xuandong Zhao and Lingjiao Chen and Haotian Ye and Sheng Liu and Zhi Huang and Daniel McFarland and James Y. Zou},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=bX3J7ho18S}\n}"
        },
        "paperhash": {
          "value": "liang|monitoring_aimodified_content_at_scale_a_case_study_on_the_impact_of_chatgpt_on_ai_conference_peer_reviews"
        }
      },
      "id": "bX3J7ho18S",
      "forum": "bX3J7ho18S",
      "license": "CC BY-NC-ND 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8675/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8675/Authors"
      ],
      "number": 8675,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8675/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706864205623,
      "cdate": 1706864205623,
      "tmdate": 1719287286190,
      "mdate": 1719287286190,
      "pdate": 1714610475233,
      "odate": 1717693095898,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics"
        },
        "authors": {
          "value": [
            "Siqi Miao",
            "Zhiyuan Lu",
            "Mia Liu",
            "Javier Duarte",
            "Pan Li"
          ]
        },
        "authorids": {
          "value": [
            "~Siqi_Miao1",
            "~Zhiyuan_Lu1",
            "~Mia_Liu1",
            "~Javier_Duarte1",
            "~Pan_Li2"
          ]
        },
        "abstract": {
          "value": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (**HEPT**), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT."
        },
        "pdf": {
          "value": "/pdf/1d8be647b0ca80c5d9d8b5ca24b7a0dc67f1b424.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nmiao2024localitysensitive,\ntitle={Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics},\nauthor={Siqi Miao and Zhiyuan Lu and Mia Liu and Javier Duarte and Pan Li},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=vJx6fld6l0}\n}"
        },
        "paperhash": {
          "value": "miao|localitysensitive_hashingbased_efficient_point_transformer_with_applications_in_highenergy_physics"
        }
      },
      "id": "vJx6fld6l0",
      "forum": "vJx6fld6l0",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8650/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8650/Authors"
      ],
      "number": 8650,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8650/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706864031434,
      "cdate": 1706864031434,
      "tmdate": 1719287285952,
      "mdate": 1719287285952,
      "pdate": 1714610474583,
      "odate": 1717693095672,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Unified Training of Universal Time Series Forecasting Transformers"
        },
        "authors": {
          "value": [
            "Gerald Woo",
            "Chenghao Liu",
            "Akshat Kumar",
            "Caiming Xiong",
            "Silvio Savarese",
            "Doyen Sahoo"
          ]
        },
        "authorids": {
          "value": [
            "~Gerald_Woo1",
            "~Chenghao_Liu1",
            "~Akshat_Kumar2",
            "~Caiming_Xiong1",
            "~Silvio_Savarese1",
            "~Doyen_Sahoo1"
          ]
        },
        "abstract": {
          "value": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of *universal forecasting*, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: (i) cross-frequency learning, (ii) accommodating an arbitrary number of variates for multivariate time series, and (iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed **M**asked Enc**o**der-based Un**i**ve**r**s**a**l T**i**me Series Forecasting Transformer (**Moirai**). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts."
        },
        "pdf": {
          "value": "/pdf/a137a5a3733a57c7ed326eed9d9293196995c310.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nwoo2024unified,\ntitle={Unified Training of Universal Time Series Forecasting Transformers},\nauthor={Gerald Woo and Chenghao Liu and Akshat Kumar and Caiming Xiong and Silvio Savarese and Doyen Sahoo},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Yd8eHMY1wz}\n}"
        },
        "paperhash": {
          "value": "woo|unified_training_of_universal_time_series_forecasting_transformers"
        }
      },
      "id": "Yd8eHMY1wz",
      "forum": "Yd8eHMY1wz",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8628/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8628/Authors"
      ],
      "number": 8628,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8628/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706863709953,
      "cdate": 1706863709953,
      "tmdate": 1719287285720,
      "mdate": 1719287285720,
      "pdate": 1714610474061,
      "odate": 1717693095489,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy"
        },
        "authors": {
          "value": [
            "Lucas Spangher",
            "Allen M. Wang",
            "Andrew Maris",
            "Myles Stapelberg",
            "Viraj Mehta",
            "Alex Saperstein",
            "Stephen Lane-Walsh",
            "Akshata Kishore Moharir",
            "Alessandro Pau",
            "Cristina Rea"
          ]
        },
        "authorids": {
          "value": [
            "~Lucas_Spangher1",
            "~Allen_M._Wang1",
            "maris@psfc.mit.edu",
            "myless@psfc.mit.edu",
            "~Viraj_Mehta1",
            "saperstein@psfc.mit.edu",
            "slwalsh@psfc.mit.edu",
            "~Akshata_Kishore_Moharir2",
            "alessandro.pau@epfl.ch",
            "crea@psfc.mit.edu"
          ]
        },
        "abstract": {
          "value": "Magnetic confinement fusion may one day provide reliable, carbon-free energy, but the field currently faces technical hurdles. In this position paper, we highlight six key research challenges in the field of fusion energy that we believe should be research priorities for the Machine Learning (ML) community because they are especially ripe for ML applications: (1) disruption prediction, (2) simulation and dynamics modeling (3) resolving partially observed data, (4) improving controls, (5) guiding experiments with optimal design, and (6) enhancing materials discovery. For each problem, we give background, review past ML work, suggest features of future models, and list challenges and idiosyncrasies facing ML development. We also discuss ongoing efforts to update the fusion data ecosystem and identify opportunities further down the line that will be enabled as fusion and its data infrastructure advance. It is our position that fusion energy offers especially exciting opportunities for ML practitioners to impact decarbonization and the future of energy."
        },
        "pdf": {
          "value": "/pdf/e0a6e8c0ab71805fdd7bf16e86d8480a96ea8139.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nspangher2024position,\ntitle={Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy},\nauthor={Lucas Spangher and Allen M. Wang and Andrew Maris and Myles Stapelberg and Viraj Mehta and Alex Saperstein and Stephen Lane-Walsh and Akshata Kishore Moharir and Alessandro Pau and Cristina Rea},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=arwP5FA2dO}\n}"
        },
        "paperhash": {
          "value": "spangher|position_opportunities_exist_for_machine_learning_in_magnetic_fusion_energy"
        }
      },
      "id": "arwP5FA2dO",
      "forum": "arwP5FA2dO",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8582/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8582/Authors"
      ],
      "number": 8582,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8582/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706863216607,
      "cdate": 1706863216607,
      "tmdate": 1719287285204,
      "mdate": 1719287285204,
      "pdate": 1714610472812,
      "odate": 1717693095082,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models"
        },
        "authors": {
          "value": [
            "Christian Schlarmann",
            "Naman Deep Singh",
            "Francesco Croce",
            "Matthias Hein"
          ]
        },
        "authorids": {
          "value": [
            "~Christian_Schlarmann1",
            "~Naman_Deep_Singh1",
            "~Francesco_Croce1",
            "~Matthias_Hein2"
          ]
        },
        "abstract": {
          "value": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub."
        },
        "pdf": {
          "value": "/pdf/500f44457db64717a500775237ac997805963ba4.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nschlarmann2024robust,\ntitle={Robust {CLIP}: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models},\nauthor={Christian Schlarmann and Naman Deep Singh and Francesco Croce and Matthias Hein},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=WLPhywf1si}\n}"
        },
        "paperhash": {
          "value": "schlarmann|robust_clip_unsupervised_adversarial_finetuning_of_vision_embeddings_for_robust_large_visionlanguage_models"
        }
      },
      "id": "WLPhywf1si",
      "forum": "WLPhywf1si",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8544/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8544/Authors"
      ],
      "number": 8544,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8544/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706862810854,
      "cdate": 1706862810854,
      "tmdate": 1719287284693,
      "mdate": 1719287284693,
      "pdate": 1714610471915,
      "odate": 1717693094766,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Less is More: on the Over-Globalizing Problem in Graph Transformers"
        },
        "authors": {
          "value": [
            "Yujie Xing",
            "Xiao Wang",
            "Yibo Li",
            "Hai Huang",
            "Chuan Shi"
          ]
        },
        "authorids": {
          "value": [
            "~Yujie_Xing2",
            "~Xiao_Wang2",
            "~Yibo_Li2",
            "~Hai_Huang9",
            "~Chuan_Shi1"
          ]
        },
        "abstract": {
          "value": "Graph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data. It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes. In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers? We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes. Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee. Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer."
        },
        "pdf": {
          "value": "/pdf/c3e25b77af010d4bb85403733de6db9620cddb23.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nxing2024less,\ntitle={Less is More: on the Over-Globalizing Problem in Graph Transformers},\nauthor={Yujie Xing and Xiao Wang and Yibo Li and Hai Huang and Chuan Shi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uKmcyyrZae}\n}"
        },
        "paperhash": {
          "value": "xing|less_is_more_on_the_overglobalizing_problem_in_graph_transformers"
        }
      },
      "id": "uKmcyyrZae",
      "forum": "uKmcyyrZae",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8483/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8483/Authors"
      ],
      "number": 8483,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8483/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706861703733,
      "cdate": 1706861703733,
      "tmdate": 1719287283969,
      "mdate": 1719287283969,
      "pdate": 1714610470362,
      "odate": 1717693094176,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
        },
        "authors": {
          "value": [
            "Jiawei Zhao",
            "Zhenyu Zhang",
            "Beidi Chen",
            "Zhangyang Wang",
            "Anima Anandkumar",
            "Yuandong Tian"
          ]
        },
        "authorids": {
          "value": [
            "~Jiawei_Zhao2",
            "~Zhenyu_Zhang4",
            "~Beidi_Chen1",
            "~Zhangyang_Wang1",
            "~Anima_Anandkumar1",
            "~Yuandong_Tian1"
          ]
        },
        "abstract": {
          "value": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies."
        },
        "pdf": {
          "value": "/pdf/21dd7e4b661c70061a5a852cb13f5481db985c79.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nzhao2024galore,\ntitle={GaLore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},\nauthor={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=hYHsrKDiX7}\n}"
        },
        "paperhash": {
          "value": "zhao|galore_memoryefficient_llm_training_by_gradient_lowrank_projection"
        }
      },
      "id": "hYHsrKDiX7",
      "forum": "hYHsrKDiX7",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8463/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8463/Authors"
      ],
      "number": 8463,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8463/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706861487537,
      "cdate": 1706861487537,
      "tmdate": 1719287283795,
      "mdate": 1719287283795,
      "pdate": 1714610469863,
      "odate": 1717693093976,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs"
        },
        "authors": {
          "value": [
            "Charlie Hou",
            "Akshat Shrivastava",
            "Hongyuan Zhan",
            "Rylan Conway",
            "Trang Le",
            "Adithya Sagar",
            "Giulia Fanti",
            "Daniel Lazar"
          ]
        },
        "authorids": {
          "value": [
            "~Charlie_Hou1",
            "~Akshat_Shrivastava1",
            "~Hongyuan_Zhan2",
            "~Rylan_Conway1",
            "~Trang_Le1",
            "adithyasagar@meta.com",
            "~Giulia_Fanti1",
            "~Daniel_Lazar1"
          ]
        },
        "abstract": {
          "value": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text."
        },
        "pdf": {
          "value": "/pdf/f008240cc2c168eb5e23534f6c9a07aa412dc41d.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nhou2024pretext,\ntitle={PrE-Text: Training Language Models on Private Federated Data in the Age of {LLM}s},\nauthor={Charlie Hou and Akshat Shrivastava and Hongyuan Zhan and Rylan Conway and Trang Le and Adithya Sagar and Giulia Fanti and Daniel Lazar},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=3WCvnkHnxV}\n}"
        },
        "paperhash": {
          "value": "hou|pretext_training_language_models_on_private_federated_data_in_the_age_of_llms"
        }
      },
      "id": "3WCvnkHnxV",
      "forum": "3WCvnkHnxV",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8128/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8128/Authors"
      ],
      "number": 8128,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8128/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706856175827,
      "cdate": 1706856175827,
      "tmdate": 1719287281020,
      "mdate": 1719287281020,
      "pdate": 1714610461509,
      "odate": 1717693089314,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation"
        },
        "authors": {
          "value": [
            "Can Yaras",
            "Peng Wang",
            "Laura Balzano",
            "Qing Qu"
          ]
        },
        "authorids": {
          "value": [
            "~Can_Yaras1",
            "~Peng_Wang23",
            "~Laura_Balzano1",
            "~Qing_Qu2"
          ]
        },
        "abstract": {
          "value": "While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data."
        },
        "pdf": {
          "value": "/pdf/b45e17afe3c2604c74b4b5729e7ee1e1da01a3bc.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nyaras2024compressible,\ntitle={Compressible Dynamics in Deep Overparameterized Low-Rank Learning \\& Adaptation},\nauthor={Can Yaras and Peng Wang and Laura Balzano and Qing Qu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uDkXoZMzBv}\n}"
        },
        "paperhash": {
          "value": "yaras|compressible_dynamics_in_deep_overparameterized_lowrank_learning_adaptation"
        }
      },
      "id": "uDkXoZMzBv",
      "forum": "uDkXoZMzBv",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission8073/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission8073/Authors"
      ],
      "number": 8073,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission8073/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706855520831,
      "cdate": 1706855520831,
      "tmdate": 1719287280285,
      "mdate": 1719287280285,
      "pdate": 1714610460103,
      "odate": 1717693087156,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Position: Technical Research and Talent is Needed for Effective AI Governance"
        },
        "authors": {
          "value": [
            "Anka Reuel",
            "Lisa Soder",
            "Benjamin Bucknall",
            "Trond Arne Undheim"
          ]
        },
        "authorids": {
          "value": [
            "~Anka_Reuel1",
            "~Lisa_Soder1",
            "~Benjamin_Bucknall1",
            "trondun@stanford.edu"
          ]
        },
        "abstract": {
          "value": "In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools. However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation. In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art. Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI."
        },
        "pdf": {
          "value": "/pdf/817d3a47d21fe1dfd8786c26da211ef1cf4e57ee.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\nreuel2024position,\ntitle={Position: Technical Research and Talent is Needed for Effective {AI} Governance},\nauthor={Anka Reuel and Lisa Soder and Benjamin Bucknall and Trond Arne Undheim},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Be2B6f0ps1}\n}"
        },
        "paperhash": {
          "value": "reuel|position_technical_research_and_talent_is_needed_for_effective_ai_governance"
        }
      },
      "id": "Be2B6f0ps1",
      "forum": "Be2B6f0ps1",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission7921/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission7921/Authors"
      ],
      "number": 7921,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission7921/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706852811334,
      "cdate": 1706852811334,
      "tmdate": 1719287279071,
      "mdate": 1719287279071,
      "pdate": 1714610456014,
      "odate": 1717693085137,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    },
    {
      "content": {
        "title": {
          "value": "Bottleneck-Minimal Indexing for Generative Document Retrieval"
        },
        "authors": {
          "value": [
            "Xin Du",
            "Lixin Xiu",
            "Kumiko Tanaka-Ishii"
          ]
        },
        "authorids": {
          "value": [
            "~Xin_Du4",
            "~Lixin_Xiu1",
            "~Kumiko_Tanaka-Ishii2"
          ]
        },
        "abstract": {
          "value": "We apply an information-theoretic perspective to reconsider generative document retrieval (GDR), in which a document $x \\in \\mathcal{X}$ is indexed by $t \\in \\mathcal{T}$, and a neural autoregressive model is trained to map queries $\\mathcal{Q}$ to $\\mathcal{T}$. GDR can be considered to involve information transmission from documents $\\mathcal{X}$ to queries $\\mathcal{Q}$, with the requirement to transmit more bits via the indexes $\\mathcal{T}$. By applying Shannon's rate-distortion theory, the optimality of indexing can be analyzed in terms of the mutual information, and the design of the indexes $\\mathcal{T}$ can then be regarded as a *bottleneck* in GDR. After reformulating GDR from this perspective, we empirically quantify the bottleneck underlying GDR. Finally, using the NQ320K and MARCO datasets, we evaluate our proposed bottleneck-minimal indexing method in comparison with various previous indexing methods, and we show that it outperforms those methods."
        },
        "pdf": {
          "value": "/pdf/9774e51f5d98fe5e6b3d28ed889cc40d66b6723a.pdf"
        },
        "venue": {
          "value": "ICML 2024 Oral"
        },
        "venueid": {
          "value": "ICML.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{\ndu2024bottleneckminimal,\ntitle={Bottleneck-Minimal Indexing for Generative Document Retrieval},\nauthor={Xin Du and Lixin Xiu and Kumiko Tanaka-Ishii},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=MFPYCvWsNR}\n}"
        },
        "paperhash": {
          "value": "du|bottleneckminimal_indexing_for_generative_document_retrieval"
        }
      },
      "id": "MFPYCvWsNR",
      "forum": "MFPYCvWsNR",
      "license": "CC BY 4.0",
      "signatures": [
        "ICML.cc/2024/Conference/Submission7920/Authors"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICML.cc/2024/Conference",
        "ICML.cc/2024/Conference/Submission7920/Authors"
      ],
      "number": 7920,
      "invitations": [
        "ICML.cc/2024/Conference/-/Submission",
        "ICML.cc/2024/Conference/-/Post_Submission",
        "ICML.cc/2024/Conference/-/Edit",
        "ICML.cc/2024/Conference/Submission7920/-/Camera_Ready_Revision"
      ],
      "domain": "ICML.cc/2024/Conference",
      "tcdate": 1706852799613,
      "cdate": 1706852799613,
      "tmdate": 1719287279049,
      "mdate": 1719287279049,
      "pdate": 1714610456012,
      "odate": 1717693085113,
      "version": 2,
      "details": {
        "replyCount": 0,
        "presentation": [
          {
            "name": "title",
            "order": 1
          },
          {
            "name": "authors",
            "order": 3
          },
          {
            "name": "authorids",
            "order": 4
          },
          {
            "name": "verify_author_list",
            "order": 4,
            "input": "checkbox"
          },
          {
            "name": "keywords",
            "order": 5
          },
          {
            "name": "TLDR",
            "order": 6,
            "fieldName": "TL;DR"
          },
          {
            "name": "abstract",
            "order": 7,
            "input": "textarea",
            "markdown": true
          },
          {
            "name": "primary_area",
            "order": 11,
            "input": "select"
          },
          {
            "name": "position_paper_track",
            "order": 12,
            "input": "radio"
          },
          {
            "name": "paper_checklist_guidelines",
            "order": 13,
            "input": "checkbox"
          },
          {
            "name": "verify_author_names",
            "order": 16,
            "input": "checkbox"
          },
          {
            "name": "no_additional_revisions",
            "order": 17,
            "input": "checkbox"
          },
          {
            "name": "pdf_appendices",
            "order": 18,
            "input": "checkbox"
          },
          {
            "name": "latest_style_file",
            "order": 19,
            "input": "checkbox"
          },
          {
            "name": "pdf",
            "order": 20
          },
          {
            "name": "paper_verification_code",
            "order": 21
          },
          {
            "name": "permissions_form",
            "order": 22
          },
          {
            "name": "supplementary_material"
          },
          {
            "name": "financial_aid"
          },
          {
            "name": "venue",
            "hidden": true
          },
          {
            "name": "venueid",
            "hidden": true
          },
          {
            "name": "_bibtex",
            "input": "textarea"
          }
        ]
      }
    }
  ],
  "count": 144,
  "fromCache": true
}