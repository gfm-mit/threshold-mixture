{"notes":[{"content":{"title":{"value":"On Discrete Prompt Optimization for Diffusion Models"},"authors":{"value":["Ruochen Wang","Ting Liu","Cho-Jui Hsieh","Boqing Gong"]},"authorids":{"value":["~Ruochen_Wang2","~Ting_Liu4","~Cho-Jui_Hsieh1","~Boqing_Gong1"]},"abstract":{"value":"This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce ``Shortcut Text Gradient\" --- an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model."},"pdf":{"value":"/pdf/e92f71b36bf501a4596d0b0a2285f14e5d15d076.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nwang2024on,\ntitle={On Discrete Prompt Optimization for Diffusion Models},\nauthor={Ruochen Wang and Ting Liu and Cho-Jui Hsieh and Boqing Gong},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Fw4fBE2rqW}\n}"},"paperhash":{"value":"wang|on_discrete_prompt_optimization_for_diffusion_models"}},"id":"Fw4fBE2rqW","forum":"Fw4fBE2rqW","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10207/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10207/Authors"],"number":10207,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10207/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706876283950,"cdate":1706876283950,"tmdate":1719287297883,"mdate":1719287297883,"pdate":1714610512428,"odate":1717693106458,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Multi-View Clustering by Inter-cluster Connectivity Guided Reward"},"authors":{"value":["Hao Dai","Yang Liu","Peng Su","Hecheng Cai","Shudong Huang","Jiancheng Lv"]},"authorids":{"value":["~Hao_Dai2","~Yang_Liu76","~Peng_Su2","~Hecheng_Cai1","~Shudong_Huang1","~Jiancheng_Lv2"]},"abstract":{"value":"Multi-view clustering has been widely explored for its effectiveness in harmonizing heterogeneity along with consistency in different views of data. Despite the significant progress made by recent works, the performance of most existing methods is heavily reliant on strong priori information regarding the true cluster number $\\textit{K}$, which is rarely feasible in real-world scenarios. In this paper, we propose a novel graph-based multi-view clustering algorithm to infer unknown $\\textit{K}$ through a graph consistency reward mechanism. To be specific, we evaluate the cluster indicator matrix during each iteration with respect to diverse $\\textit{K}$. We formulate the inference process of unknown $\\textit{K}$ as a parsimonious reinforcement learning paradigm, where the reward is measured by inter-cluster connectivity. As a result, our approach is capable of independently producing the final clustering result, free from the input of a predefined cluster number. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach in comparison to existing state-of-the-art methods."},"pdf":{"value":"/pdf/3fad1c7d0f147e1c57ba5137d9ff021e896a3883.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\ndai2024multiview,\ntitle={Multi-View Clustering by Inter-cluster Connectivity Guided Reward},\nauthor={Hao Dai and Yang Liu and Peng Su and Hecheng Cai and Shudong Huang and Jiancheng Lv},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uEx2bSAJu8}\n}"},"paperhash":{"value":"dai|multiview_clustering_by_intercluster_connectivity_guided_reward"}},"id":"uEx2bSAJu8","forum":"uEx2bSAJu8","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10190/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10190/Authors"],"number":10190,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10190/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875254062,"cdate":1706875254062,"tmdate":1719287297876,"mdate":1719287297876,"pdate":1714610512102,"odate":1717693106457,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning"},"authors":{"value":["Michal Nauman","Michał Bortkiewicz","Piotr Miłoś","Tomasz Trzcinski","Mateusz Ostaszewski","Marek Cygan"]},"authorids":{"value":["~Michal_Nauman1","~Michał_Bortkiewicz1","~Piotr_Miłoś1","~Tomasz_Trzcinski2","~Mateusz_Ostaszewski1","~Marek_Cygan1"]},"abstract":{"value":"Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks, measuring training metrics related to overestimation, overfitting, and plasticity loss — issues that motivate the examined regularization techniques. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. Notably, a simple Soft Actor-Critic agent, appropriately regularized, reliably finds a better-performing policy within the training regime, which previously was achieved mainly through model-based approaches."},"pdf":{"value":"/pdf/d561885e9c79779a955cf26ffe88fc4616faa08a.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nnauman2024overestimation,\ntitle={Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning},\nauthor={Michal Nauman and Micha{\\l} Bortkiewicz and Piotr Mi{\\l}o{\\'s} and Tomasz Trzcinski and Mateusz Ostaszewski and Marek Cygan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=5vZzmCeTYu}\n}"},"paperhash":{"value":"nauman|overestimation_overfitting_and_plasticity_in_actorcritic_the_bitter_lesson_of_reinforcement_learning"}},"id":"5vZzmCeTYu","forum":"5vZzmCeTYu","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10168/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10168/Authors"],"number":10168,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10168/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875176729,"cdate":1706875176729,"tmdate":1719287297848,"mdate":1719287297848,"pdate":1714610511647,"odate":1717693106439,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Copyright Traps for Large Language Models"},"authors":{"value":["Matthieu Meeus","Igor Shilov","Manuel Faysse","Yves-Alexandre de Montjoye"]},"authorids":{"value":["~Matthieu_Meeus1","~Igor_Shilov1","~Manuel_Faysse1","~Yves-Alexandre_de_Montjoye1"]},"abstract":{"value":"Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize significantly, we hypothesize - and later confirm - that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. Beyond copyright applications, our findings contribute to the study of LLM memorization: the randomized controlled setup enables us to draw causal relationships between memorization and certain sequence properties such as repetition in model training data and perplexity."},"pdf":{"value":"/pdf/0be538a49e53626f9a6e2f02f36963641cb44766.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nmeeus2024copyright,\ntitle={Copyright Traps for Large Language Models},\nauthor={Matthieu Meeus and Igor Shilov and Manuel Faysse and Yves-Alexandre de Montjoye},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=LDq1JPdc55}\n}"},"paperhash":{"value":"meeus|copyright_traps_for_large_language_models"}},"id":"LDq1JPdc55","forum":"LDq1JPdc55","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10167/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10167/Authors"],"number":10167,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10167/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875176105,"cdate":1706875176105,"tmdate":1719287297810,"mdate":1719287297810,"pdate":1714610511626,"odate":1717693106368,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Implicit meta-learning may lead language models to trust more reliable sources"},"authors":{"value":["Dmitrii Krasheninnikov","Egor Krasheninnikov","Bruno Kacper Mlodozeniec","Tegan Maharaj","David Krueger"]},"authorids":{"value":["~Dmitrii_Krasheninnikov1","~Egor_Krasheninnikov1","~Bruno_Kacper_Mlodozeniec2","~Tegan_Maharaj1","~David_Krueger1"]},"abstract":{"value":"We demonstrate that large language models (LLMs) may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\"tags\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to **implicit meta-learning (IML)**: in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about the capabilities, risks, and controllability of future AI systems."},"pdf":{"value":"/pdf/f9ff7df1c721b133862e6327f51393e368a6dd80.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nkrasheninnikov2024implicit,\ntitle={Implicit meta-learning may lead language models to trust more reliable sources},\nauthor={Dmitrii Krasheninnikov and Egor Krasheninnikov and Bruno Kacper Mlodozeniec and Tegan Maharaj and David Krueger},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Fzp1DRzCIN}\n}"},"paperhash":{"value":"krasheninnikov|implicit_metalearning_may_lead_language_models_to_trust_more_reliable_sources"}},"id":"Fzp1DRzCIN","forum":"Fzp1DRzCIN","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10166/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10166/Authors"],"number":10166,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10166/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875174288,"cdate":1706875174288,"tmdate":1721109270606,"mdate":1721109270606,"pdate":1714610511585,"odate":1717693106367,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming"},"authors":{"value":["Hany Hamed","Subin Kim","Dongyeong Kim","Jaesik Yoon","Sungjin Ahn"]},"authorids":{"value":["~Hany_Hamed1","~Subin_Kim4","~Dongyeong_Kim1","~Jaesik_Yoon1","~Sungjin_Ahn1"]},"abstract":{"value":"Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question *whether and how an agent can ``*dream better*''* in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called **Dr. Strategy**, which is equipped with a novel **Dr**eaming **Strategy**. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks."},"pdf":{"value":"/pdf/2df0ba6c3287a3539533864714ad27ac435f730c.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nhamed2024dr,\ntitle={Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming},\nauthor={Hany Hamed and Subin Kim and Dongyeong Kim and Jaesik Yoon and Sungjin Ahn},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=HsseRq2FAx}\n}"},"paperhash":{"value":"hamed|dr_strategy_modelbased_generalist_agents_with_strategic_dreaming"}},"id":"HsseRq2FAx","forum":"HsseRq2FAx","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10161/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10161/Authors"],"number":10161,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10161/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875161526,"cdate":1706875161526,"tmdate":1719287297734,"mdate":1719287297734,"pdate":1714610511420,"odate":1717693106342,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"When Will Gradient Regularization Be Harmful?"},"authors":{"value":["Yang Zhao","Hao Zhang","Xiuyuan Hu"]},"authorids":{"value":["~Yang_Zhao11","~Hao_Zhang37","~Xiuyuan_Hu1"]},"abstract":{"value":"Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp."},"pdf":{"value":"/pdf/c9d36d02b49f47ef4263bc76d1cd028f1655c02a.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nzhao2024when,\ntitle={When Will Gradient Regularization Be Harmful?},\nauthor={Yang Zhao and Hao Zhang and Xiuyuan Hu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=60vC1FY0dZ}\n}"},"paperhash":{"value":"zhao|when_will_gradient_regularization_be_harmful"}},"id":"60vC1FY0dZ","forum":"60vC1FY0dZ","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10159/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10159/Authors"],"number":10159,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10159/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875158970,"cdate":1706875158970,"tmdate":1719287297742,"mdate":1719287297742,"pdate":1714610511403,"odate":1717693106279,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond"},"authors":{"value":["Dingzhi Yu","Yunuo Cai","Wei Jiang","Lijun Zhang"]},"authorids":{"value":["~Dingzhi_Yu1","~Yunuo_Cai1","~Wei_Jiang8","~Lijun_Zhang1"]},"abstract":{"value":"In this paper, we investigate the empirical counterpart of Group Distributionally Robust Optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a *two-level* finite-sum convex-concave minimax optimization problem and develop an algorithm called ALEG to benefit from its special structure. ALEG is a double-looped stochastic primal-dual algorithm that incorporates variance reduction techniques into a modified mirror prox routine. To exploit the two-level finite-sum structure, we propose a simple group sampling strategy to construct the stochastic gradient with a smaller Lipschitz constant and then perform variance reduction for all groups. Theoretical analysis shows that ALEG achieves $\\varepsilon$-accuracy within a computation complexity of $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, where $\\bar n$ is the average number of samples among $m$ groups. Notably, our approach outperforms the state-of-the-art method by a factor of $\\sqrt{m}$. Based on ALEG, we further develop a two-stage optimization algorithm called ALEM to deal with the empirical Minimax Excess Risk Optimization (MERO) problem. The computation complexity of ALEM nearly matches that of ALEG, surpassing the rates of existing methods."},"pdf":{"value":"/pdf/f287b77bba02edd7f0307d9353e94fb1a90cc75c.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nyu2024efficient,\ntitle={Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond},\nauthor={Dingzhi Yu and Yunuo Cai and Wei Jiang and Lijun Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=pOJbk4Nzmi}\n}"},"paperhash":{"value":"yu|efficient_algorithms_for_empirical_group_distributionally_robust_optimization_and_beyond"}},"id":"pOJbk4Nzmi","forum":"pOJbk4Nzmi","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10129/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10129/Authors"],"number":10129,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10129/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875098358,"cdate":1706875098358,"tmdate":1719287297657,"mdate":1719287297657,"pdate":1714610510783,"odate":1717693106276,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Evaluation of Trajectory Distribution Predictions with Energy Score"},"authors":{"value":["Novin Shahroudi","Mihkel Lepson","Meelis Kull"]},"authorids":{"value":["~Novin_Shahroudi1","~Mihkel_Lepson1","~Meelis_Kull1"]},"abstract":{"value":"Predicting the future trajectory of surrounding objects is inherently uncertain and vital in the safe and reliable planning of autonomous systems such as in self-driving cars. Although trajectory prediction models have become increasingly sophisticated in dealing with the complexities of spatiotemporal data, the evaluation methods used to assess these models have not kept pace. \"Minimum of N\" is a common family of metrics used to assess the rich outputs of such models. We critically examine the Minimum of N within the proper scoring rules framework to show that it is not strictly proper and demonstrate how that could lead to a misleading assessment of multimodal trajectory predictions. As an alternative, we propose using Energy Score-based evaluation measures, leveraging their proven propriety for a more reliable evaluation of trajectory distribution predictions."},"pdf":{"value":"/pdf/ebcc0b1955e28327a9562073b6524f7ea3ad163b.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nshahroudi2024evaluation,\ntitle={Evaluation of Trajectory Distribution Predictions with Energy Score},\nauthor={Novin Shahroudi and Mihkel Lepson and Meelis Kull},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=FCmWhJQ14I}\n}"},"paperhash":{"value":"shahroudi|evaluation_of_trajectory_distribution_predictions_with_energy_score"}},"id":"FCmWhJQ14I","forum":"FCmWhJQ14I","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10124/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10124/Authors"],"number":10124,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10124/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875084991,"cdate":1706875084991,"tmdate":1719287297652,"mdate":1719287297652,"pdate":1714610510681,"odate":1717693106247,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Causal Discovery with Fewer Conditional Independence Tests"},"authors":{"value":["Kirankumar Shiragur","Jiaqi Zhang","Caroline Uhler"]},"authorids":{"value":["~Kirankumar_Shiragur1","~Jiaqi_Zhang2","~Caroline_Uhler1"]},"abstract":{"value":"Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an _exponential_ number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a _polynomial_ number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions."},"pdf":{"value":"/pdf/23331b236f09db1c4293895ed7d92c5d7c76991f.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nshiragur2024causal,\ntitle={Causal Discovery with Fewer Conditional Independence Tests},\nauthor={Kirankumar Shiragur and Jiaqi Zhang and Caroline Uhler},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=HpT19AKddu}\n}"},"paperhash":{"value":"shiragur|causal_discovery_with_fewer_conditional_independence_tests"}},"id":"HpT19AKddu","forum":"HpT19AKddu","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10108/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10108/Authors"],"number":10108,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10108/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875056438,"cdate":1706875056438,"tmdate":1719287297606,"mdate":1719287297606,"pdate":1714610510229,"odate":1717693106187,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion"},"authors":{"value":["Xuantong LIU","Tianyang Hu","Wenjia Wang","Kenji Kawaguchi","Yuan Yao"]},"authorids":{"value":["~Xuantong_LIU1","~Tianyang_Hu1","~Wenjia_Wang2","~Kenji_Kawaguchi1","~Yuan_Yao1"]},"abstract":{"value":"As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench. The code is available at https://github.com/Pepper-lll/VLMinv."},"pdf":{"value":"/pdf/a24fa3e6f02bf1e75586a0204f1d38fd01d2c784.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nliu2024referee,\ntitle={Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion},\nauthor={Xuantong LIU and Tianyang Hu and Wenjia Wang and Kenji Kawaguchi and Yuan Yao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=hZ0fWhgVch}\n}"},"paperhash":{"value":"liu|referee_can_play_an_alternative_approach_to_conditional_generation_via_model_inversion"}},"id":"hZ0fWhgVch","forum":"hZ0fWhgVch","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10107/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10107/Authors"],"number":10107,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10107/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875055614,"cdate":1706875055614,"tmdate":1719287297582,"mdate":1719287297582,"pdate":1714610510191,"odate":1717693106163,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"},"authors":{"value":["Mantas Mazeika","Long Phan","Xuwang Yin","Andy Zou","Zifan Wang","Norman Mu","Elham Sakhaee","Nathaniel Li","Steven Basart","Bo Li","David Forsyth","Dan Hendrycks"]},"authorids":{"value":["~Mantas_Mazeika3","~Long_Phan1","~Xuwang_Yin2","~Andy_Zou1","~Zifan_Wang1","~Norman_Mu1","~Elham_Sakhaee3","~Nathaniel_Li1","~Steven_Basart1","~Bo_Li19","~David_Forsyth1","~Dan_Hendrycks1"]},"abstract":{"value":"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench."},"pdf":{"value":"/pdf/cc74d66380e695bd4620349eb2b697002bec8602.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nmazeika2024harmbench,\ntitle={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},\nauthor={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=f3TUipYU3U}\n}"},"paperhash":{"value":"mazeika|harmbench_a_standardized_evaluation_framework_for_automated_red_teaming_and_robust_refusal"}},"id":"f3TUipYU3U","forum":"f3TUipYU3U","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10106/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10106/Authors"],"number":10106,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10106/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875052745,"cdate":1706875052745,"tmdate":1719287297553,"mdate":1719287297553,"pdate":1714610510161,"odate":1717693106161,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Adversarial Attacks on Combinatorial Multi-Armed Bandits"},"authors":{"value":["Rishab Balasubramanian","Jiawei Li","Prasad Tadepalli","Huazheng Wang","Qingyun Wu","Haoyu Zhao"]},"authorids":{"value":["~Rishab_Balasubramanian1","~Jiawei_Li10","~Prasad_Tadepalli3","~Huazheng_Wang1","~Qingyun_Wu2","~Haoyu_Zhao1"]},"abstract":{"value":"We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path."},"pdf":{"value":"/pdf/01014dddf5fd9478dc47547705302f5fdb252684.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nbalasubramanian2024adversarial,\ntitle={Adversarial Attacks on Combinatorial Multi-Armed Bandits},\nauthor={Rishab Balasubramanian and Jiawei Li and Prasad Tadepalli and Huazheng Wang and Qingyun Wu and Haoyu Zhao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=0tPBk24xNj}\n}"},"paperhash":{"value":"balasubramanian|adversarial_attacks_on_combinatorial_multiarmed_bandits"}},"id":"0tPBk24xNj","forum":"0tPBk24xNj","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10092/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10092/Authors"],"number":10092,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10092/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875016306,"cdate":1706875016306,"tmdate":1719287297530,"mdate":1719287297530,"pdate":1714610509836,"odate":1717693106114,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers"},"authors":{"value":["Brian K Chen","Tianyang Hu","Hui Jin","Hwee Kuan Lee","Kenji Kawaguchi"]},"authorids":{"value":["~Brian_K_Chen1","~Tianyang_Hu1","~Hui_Jin1","~Hwee_Kuan_Lee1","~Kenji_Kawaguchi1"]},"abstract":{"value":"In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms."},"pdf":{"value":"/pdf/25c3e1583fb72822e03302699b39755b701738a4.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nchen2024exact,\ntitle={Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers},\nauthor={Brian K Chen and Tianyang Hu and Hui Jin and Hwee Kuan Lee and Kenji Kawaguchi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=LVF4P1NNwO}\n}"},"paperhash":{"value":"chen|exact_conversion_of_incontext_learning_to_model_weights_in_linearizedattention_transformers"}},"id":"LVF4P1NNwO","forum":"LVF4P1NNwO","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10091/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10091/Authors"],"number":10091,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10091/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875015148,"cdate":1706875015148,"tmdate":1719287297502,"mdate":1719287297502,"pdate":1714610509804,"odate":1717693106072,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems"},"authors":{"value":["Bonan Zhang","Chia-Yu Chen","Naveen Verma"]},"authorids":{"value":["~Bonan_Zhang1","~Chia-Yu_Chen2","~Naveen_Verma1"]},"abstract":{"value":"In-memory computing (IMC) has emerged as a promising solution to address both computation and data-movement challenges, by performing computation on data in-place directly in the memory array. IMC typically relies on analog operation, which makes analog-to-digital converters (ADCs) necessary, for converting results back to the digital domain. However, ADCs maintain computational efficiency by having limited precision, leading to substantial quantization errors in compute outputs. This work proposes RAOQ (Reshape and Adapt for Output Quantization) to overcome this issue, which comprises two classes of mechanisms including: 1) mitigating ADC quantization error by adjusting the statistics of activations and weights, through an activation-shifting approach (A-shift) and a weight reshaping technique (W-reshape); 2) adapting AI models to better tolerate ADC quantization through a bit augmentation method (BitAug), complemented by the introduction of ADC-LoRA, a low-rank approximation technique, to reduce the training overhead. RAOQ demonstrates consistently high performance across different scales and domains of neural network models for computer vision and natural language processing (NLP) tasks at various bit precisions, achieving state-of-the-art results with practical IMC implementations."},"pdf":{"value":"/pdf/e2b51f7bd7794737a678b389a2d7b8183cf3bca1.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nzhang2024reshape,\ntitle={Reshape and Adapt for Output Quantization ({RAOQ}): Quantization-aware Training for In-memory Computing Systems},\nauthor={Bonan Zhang and Chia-Yu Chen and Naveen Verma},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=fM9xTkpAdu}\n}"},"paperhash":{"value":"zhang|reshape_and_adapt_for_output_quantization_raoq_quantizationaware_training_for_inmemory_computing_systems"}},"id":"fM9xTkpAdu","forum":"fM9xTkpAdu","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10086/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10086/Authors"],"number":10086,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10086/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706875003693,"cdate":1706875003693,"tmdate":1719287297474,"mdate":1719287297474,"pdate":1714610509698,"odate":1717693105996,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method"},"authors":{"value":["Qinghua Tao","Francesco Tonin","Alex Lambert","Yingyi Chen","Panagiotis Patrinos","Johan Suykens"]},"authorids":{"value":["~Qinghua_Tao1","~Francesco_Tonin1","~Alex_Lambert1","~Yingyi_Chen3","~Panagiotis_Patrinos1","~Johan_Suykens1"]},"abstract":{"value":"In contrast with Mercer kernel-based approaches as used e.g. in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed. However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning. In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps. The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD. ii) Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nyström method through a finite sample approximation to speed up training. iii) We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks."},"pdf":{"value":"/pdf/b0796df605b106d5168c1bddbd4347df1a4f65bc.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\ntao2024learning,\ntitle={Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel {SVD} and Nystr\\\"om method},\nauthor={Qinghua Tao and Francesco Tonin and Alex Lambert and Yingyi Chen and Panagiotis Patrinos and Johan Suykens},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Gp0xZDmrA2}\n}"},"paperhash":{"value":"tao|learning_in_feature_spaces_via_coupled_covariances_asymmetric_kernel_svd_and_nyström_method"}},"id":"Gp0xZDmrA2","forum":"Gp0xZDmrA2","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10082/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10082/Authors"],"number":10082,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10082/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874994790,"cdate":1706874994790,"tmdate":1719287297469,"mdate":1719287297469,"pdate":1714610509559,"odate":1717693105945,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent"},"authors":{"value":["Yingru Li","Jiawei Xu","Lei Han","Zhi-Quan Luo"]},"authorids":{"value":["~Yingru_Li1","~Jiawei_Xu1","~Lei_Han1","~Zhi-Quan_Luo1"]},"abstract":{"value":"We propose HyperAgent, a reinforcement learning (RL) algorithm based on the hypermodel framework for exploration in RL. HyperAgent allows for the efficient incremental approximation of posteriors associated with an optimal action-value function ($Q^\\star$) without the need for conjugacy and follows the greedy policies w.r.t. these approximate posterior samples. We demonstrate that HyperAgent offers robust performance in large-scale deep RL benchmarks. It can solve Deep Sea hard exploration problems with episodes that optimally scale with problem size and exhibits significant efficiency gains in the Atari suite. Implementing HyperAgent requires minimal code addition to well-established deep RL frameworks like DQN. We theoretically prove that, under tabular assumptions, HyperAgent achieves logarithmic per-step computational complexity while attaining sublinear regret, matching the best known randomized tabular RL algorithm."},"pdf":{"value":"/pdf/21d1bbc8276fc96c77ad2ea600b16c1dfcd198ab.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nli2024qstar,\ntitle={Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent},\nauthor={Yingru Li and Jiawei Xu and Lei Han and Zhi-Quan Luo},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=OF7e0w1uon}\n}"},"paperhash":{"value":"li|qstar_meets_scalable_posterior_sampling_bridging_theory_and_practice_via_hyperagent"}},"id":"OF7e0w1uon","forum":"OF7e0w1uon","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10071/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10071/Authors"],"number":10071,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10071/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874959863,"cdate":1706874959863,"tmdate":1719287297401,"mdate":1719287297401,"pdate":1714610509288,"odate":1717693105868,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Simplicity Bias via Global Convergence of Sharpness Minimization"},"authors":{"value":["Khashayar Gatmiry","Zhiyuan Li","Sashank J. Reddi","Stefanie Jegelka"]},"authorids":{"value":["~Khashayar_Gatmiry1","~Zhiyuan_Li2","~Sashank_J._Reddi1","~Stefanie_Jegelka3"]},"abstract":{"value":"The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest."},"pdf":{"value":"/pdf/294689f06d4ca7f2793a449ff2241b428bd972ed.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\ngatmiry2024simplicity,\ntitle={Simplicity Bias via Global Convergence of Sharpness Minimization},\nauthor={Khashayar Gatmiry and Zhiyuan Li and Sashank J. Reddi and Stefanie Jegelka},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=VUTyzH63Xa}\n}"},"paperhash":{"value":"gatmiry|simplicity_bias_via_global_convergence_of_sharpness_minimization"}},"id":"VUTyzH63Xa","forum":"VUTyzH63Xa","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10069/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10069/Authors"],"number":10069,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10069/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874957801,"cdate":1706874957801,"tmdate":1719287297399,"mdate":1719287297399,"pdate":1714610509272,"odate":1717693105837,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"STEER: Assessing the Economic Rationality of Large Language Models"},"authors":{"value":["Narun Krishnamurthi Raman","Taylor Lundy","Samuel Joseph Amouyal","Yoav Levine","Kevin Leyton-Brown","Moshe Tennenholtz"]},"authorids":{"value":["~Narun_Krishnamurthi_Raman1","~Taylor_Lundy1","~Samuel_Joseph_Amouyal1","~Yoav_Levine1","~Kevin_Leyton-Brown1","~Moshe_Tennenholtz1"]},"abstract":{"value":"There is increasing interest in using LLMs as decision-making \"agents\". Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions---and more broadly, determining whether an LLM agent is reliable enough to be trusted---requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card\". Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior."},"pdf":{"value":"/pdf/be8b2a0736d866318c6d6137b6a5cd7b256ac09b.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nraman2024steer,\ntitle={{STEER}: Assessing the Economic Rationality of Large Language Models},\nauthor={Narun Krishnamurthi Raman and Taylor Lundy and Samuel Joseph Amouyal and Yoav Levine and Kevin Leyton-Brown and Moshe Tennenholtz},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=nU1mtFDtMX}\n}"},"paperhash":{"value":"raman|steer_assessing_the_economic_rationality_of_large_language_models"}},"id":"nU1mtFDtMX","forum":"nU1mtFDtMX","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10056/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10056/Authors"],"number":10056,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10056/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874926230,"cdate":1706874926230,"tmdate":1719287297368,"mdate":1719287297368,"pdate":1714610508892,"odate":1717693105798,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws"},"authors":{"value":["Nikhil Sardana","Jacob Portes","Sasha Doubov","Jonathan Frankle"]},"authorids":{"value":["~Nikhil_Sardana1","~Jacob_Portes1","~Sasha_Doubov1","~Jonathan_Frankle1"]},"abstract":{"value":"Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ($\\sim$1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges."},"pdf":{"value":"/pdf/77e05ffe052617301d4740a04e86ccb07081d696.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nsardana2024beyond,\ntitle={Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},\nauthor={Nikhil Sardana and Jacob Portes and Sasha Doubov and Jonathan Frankle},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=0bmXrtTDUu}\n}"},"paperhash":{"value":"sardana|beyond_chinchillaoptimal_accounting_for_inference_in_language_model_scaling_laws"}},"id":"0bmXrtTDUu","forum":"0bmXrtTDUu","license":"CC BY-NC 4.0","signatures":["ICML.cc/2024/Conference/Submission10055/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10055/Authors"],"number":10055,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10055/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874918755,"cdate":1706874918755,"tmdate":1719287297320,"mdate":1719287297320,"pdate":1714610508869,"odate":1717693105796,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts"},"authors":{"value":["Ruochen Wang","Sohyun An","Minhao Cheng","Tianyi Zhou","Sung Ju Hwang","Cho-Jui Hsieh"]},"authorids":{"value":["~Ruochen_Wang2","~Sohyun_An1","~Minhao_Cheng1","~Tianyi_Zhou1","~Sung_Ju_Hwang1","~Cho-Jui_Hsieh1"]},"abstract":{"value":"Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks."},"pdf":{"value":"/pdf/54ff0b135e6b5630849ddfa72a66d987e7d0dff9.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nwang2024one,\ntitle={One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts},\nauthor={Ruochen Wang and Sohyun An and Minhao Cheng and Tianyi Zhou and Sung Ju Hwang and Cho-Jui Hsieh},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=edHLN40DWu}\n}"},"paperhash":{"value":"wang|one_prompt_is_not_enough_automated_construction_of_a_mixtureofexpert_prompts"}},"id":"edHLN40DWu","forum":"edHLN40DWu","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10053/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10053/Authors"],"number":10053,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10053/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874913218,"cdate":1706874913218,"tmdate":1719287297298,"mdate":1719287297298,"pdate":1714610508802,"odate":1717693105742,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements"},"authors":{"value":["Kyuwon Kim","Donghwan Kim"]},"authorids":{"value":["~Kyuwon_Kim1","~Donghwan_Kim2"]},"abstract":{"value":"In nonconvex-nonconcave minimax optimization, two-timescale gradient methods have shown their potential to find local minimax (optimal) points, provided that the timescale separation between the min and the max player is sufficiently large. However, existing two-timescale variants of gradient descent ascent and extragradient methods face two shortcomings, especially when we search for non-strict local minimax points that are prevalent in modern overparameterized setting. In specific, (1) these methods can be unstable at some non-strict local minimax points even with sufficiently large timescale separation, and even (2) computing a proper amount of timescale separation is infeasible in practice. To remedy these two issues, we propose to incorporate two simple but provably effective schemes, double-step alternating update and increasing timescale separation, into the two-timescale extragradient method, respectively. Under mild conditions, we show that the proposed methods converge to non-strict local minimax points that all existing two-timescale methods fail to converge."},"pdf":{"value":"/pdf/7075f40d146aa11801c2b028fc04e41464c63f51.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nkim2024doublestep,\ntitle={Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements},\nauthor={Kyuwon Kim and Donghwan Kim},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=nUVForc3VP}\n}"},"paperhash":{"value":"kim|doublestep_alternating_extragradient_with_increasing_timescale_separation_for_finding_local_minimax_points_provable_improvements"}},"id":"nUVForc3VP","forum":"nUVForc3VP","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10043/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10043/Authors"],"number":10043,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10043/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874871196,"cdate":1706874871196,"tmdate":1719287297260,"mdate":1719287297260,"pdate":1714610508607,"odate":1717693105703,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration"},"authors":{"value":["Zhongzhi Yu","Zheng Wang","Yonggan Fu","Huihong Shi","Khalid Shaikh","Yingyan Celine Lin"]},"authorids":{"value":["~Zhongzhi_Yu1","~Zheng_Wang38","~Yonggan_Fu1","~Huihong_Shi1","kshaikh6@gatech.edu","~Yingyan_Celine_Lin1"]},"abstract":{"value":"Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to $7.30\\%$ in accuracy across different datasets when applied to Llama-30B."},"pdf":{"value":"/pdf/40345a383990aaae81fe2b780c09093d87a48e09.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nyu2024unveiling,\ntitle={Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration},\nauthor={Zhongzhi Yu and Zheng Wang and Yonggan Fu and Huihong Shi and Khalid Shaikh and Yingyan Celine Lin},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=DLTjFFiuUJ}\n}"},"paperhash":{"value":"yu|unveiling_and_harnessing_hidden_attention_sinks_enhancing_large_language_models_without_training_through_attention_calibration"}},"id":"DLTjFFiuUJ","forum":"DLTjFFiuUJ","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10042/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10042/Authors"],"number":10042,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10042/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874869857,"cdate":1706874869857,"tmdate":1719287297238,"mdate":1719287297238,"pdate":1714610508593,"odate":1717693105675,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision"},"authors":{"value":["Zhuo Chen","Jacob McCarran","Esteban Vizcaino","Marin Soljacic","Di Luo"]},"authorids":{"value":["~Zhuo_Chen8","~Jacob_McCarran1","~Esteban_Vizcaino1","~Marin_Soljacic1","~Di_Luo1"]},"abstract":{"value":"Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the *Time-Evolving Natural Gradient (TENG)*, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving *machine precision* in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation."},"pdf":{"value":"/pdf/e428bebd4b5af90b728217e0591dbc18bc78b2e7.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nchen2024teng,\ntitle={{TENG}: Time-Evolving Natural Gradient for Solving {PDE}s With Deep Neural Nets Toward Machine Precision},\nauthor={Zhuo Chen and Jacob McCarran and Esteban Vizcaino and Marin Soljacic and Di Luo},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=v1I4zRAjMb}\n}"},"paperhash":{"value":"chen|teng_timeevolving_natural_gradient_for_solving_pdes_with_deep_neural_nets_toward_machine_precision"}},"id":"v1I4zRAjMb","forum":"v1I4zRAjMb","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10038/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10038/Authors"],"number":10038,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10038/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874853741,"cdate":1706874853741,"tmdate":1719287297205,"mdate":1719287297205,"pdate":1714610508503,"odate":1717693105650,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}},{"content":{"title":{"value":"Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking"},"authors":{"value":["Yongxin Li","Mengyuan Liu","You Wu","Xucheng Wang","Xiangyang Yang","Shuiwang Li"]},"authorids":{"value":["~Yongxin_Li1","~Mengyuan_Liu3","~You_Wu8","~Xucheng_Wang1","~Xiangyang_Yang1","~Shuiwang_Li1"]},"abstract":{"value":"Harnessing transformer-based models, visual tracking has made substantial strides. However, the sluggish performance of current trackers limits their practicality on devices with constrained computational capabilities, especially for real-time unmanned aerial vehicle (UAV) tracking. Addressing this challenge, we introduce AVTrack, an adaptive computation framework tailored to selectively activate transformer blocks for real-time UAV tracking in this work. Our novel Activation Module (AM) dynamically optimizes ViT architecture, selectively engaging relevant components and enhancing inference efficiency without compromising much tracking performance. Moreover, we bolster the effectiveness of ViTs, particularly in addressing challenges arising from extreme changes in viewing angles commonly encountered in UAV tracking, by learning view-invariant representations through mutual information maximization. Extensive experiments on five tracking benchmarks affirm the effectiveness and versatility of our approach, positioning it as a state-of-the-art solution in visual tracking. Code is released at: https://github.com/wuyou3474/AVTrack."},"pdf":{"value":"/pdf/3dc52c7d2c6724b97412afaf3106f51c6d2f4377.pdf"},"venue":{"value":"ICML 2024 Poster"},"venueid":{"value":"ICML.cc/2024/Conference"},"_bibtex":{"value":"@inproceedings{\nli2024learning,\ntitle={Learning Adaptive and View-Invariant Vision Transformer for Real-Time {UAV} Tracking},\nauthor={Yongxin Li and Mengyuan Liu and You Wu and Xucheng Wang and Xiangyang Yang and Shuiwang Li},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=eaNLvrP8n1}\n}"},"paperhash":{"value":"li|learning_adaptive_and_viewinvariant_vision_transformer_for_realtime_uav_tracking"}},"id":"eaNLvrP8n1","forum":"eaNLvrP8n1","license":"CC BY 4.0","signatures":["ICML.cc/2024/Conference/Submission10036/Authors"],"readers":["everyone"],"writers":["ICML.cc/2024/Conference","ICML.cc/2024/Conference/Submission10036/Authors"],"number":10036,"invitations":["ICML.cc/2024/Conference/-/Submission","ICML.cc/2024/Conference/-/Post_Submission","ICML.cc/2024/Conference/-/Edit","ICML.cc/2024/Conference/Submission10036/-/Camera_Ready_Revision"],"domain":"ICML.cc/2024/Conference","tcdate":1706874846034,"cdate":1706874846034,"tmdate":1719287297169,"mdate":1719287297169,"pdate":1714610508410,"odate":1717693105612,"version":2,"details":{"replyCount":0,"presentation":[{"name":"title","order":1},{"name":"authors","order":3},{"name":"authorids","order":4},{"name":"verify_author_list","order":4,"input":"checkbox"},{"name":"keywords","order":5},{"name":"TLDR","order":6,"fieldName":"TL;DR"},{"name":"abstract","order":7,"input":"textarea","markdown":true},{"name":"primary_area","order":11,"input":"select"},{"name":"position_paper_track","order":12,"input":"radio"},{"name":"paper_checklist_guidelines","order":13,"input":"checkbox"},{"name":"verify_author_names","order":16,"input":"checkbox"},{"name":"no_additional_revisions","order":17,"input":"checkbox"},{"name":"pdf_appendices","order":18,"input":"checkbox"},{"name":"latest_style_file","order":19,"input":"checkbox"},{"name":"pdf","order":20},{"name":"paper_verification_code","order":21},{"name":"permissions_form","order":22},{"name":"supplementary_material"},{"name":"financial_aid"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","input":"textarea"}]}}],"count":2275,"fromCache":true}